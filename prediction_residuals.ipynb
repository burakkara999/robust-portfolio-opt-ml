{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "29f230bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from asset_data_module import read_close_prices_all_merged\n",
    "from features import make_feature_windows\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "66347c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((312, 30), 126)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markets = ['dow30']\n",
    "# markets = ['commodities']\n",
    "# markets = ['dow30', 'commodities', 'bonds', 'funds_mini']\n",
    "# start_date, end_date = \"2022-01-01\", \"2025-11-28\"\n",
    "start_date, end_date = \"2024-09-01\", \"2025-11-28\"\n",
    "\n",
    "_, close_df = read_close_prices_all_merged(markets, after_date=start_date)\n",
    "close_df = close_df.loc[:end_date]\n",
    "\n",
    "rolling = make_feature_windows(\n",
    "    close_prices=close_df,\n",
    "    lookback=30,\n",
    "    horizon=1,\n",
    "    days_per_week=2\n",
    ")\n",
    "close_df.shape, len(rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "358d1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_from_windows(windows, x_key=\"past_weekly_returns\", y_key=\"y_ret\", residual_over_sample_mean=True, baseline_key=\"past_weekly_returns\"):\n",
    "    X_list, y_list, mu_list = [], [], []\n",
    "    meta_rows = []\n",
    "\n",
    "    for w_idx, w in enumerate(windows):\n",
    "\n",
    "        # ---- X ----\n",
    "        if x_key == \"past_weekly_returns\":\n",
    "            X_df = w[x_key].T                 # assets x lookback\n",
    "        elif x_key == \"X_feat\":\n",
    "            X_df = w[x_key]                   # assets x features\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown x_key: {x_key}\")\n",
    "\n",
    "        # ---- y ----\n",
    "        y_ser = w[y_key]\n",
    "        if not isinstance(y_ser, pd.Series):\n",
    "            y_ser = pd.Series(y_ser, index=X_df.index)\n",
    "\n",
    "        # ---- baseline mu (always computed from past period returns) ----\n",
    "        # past_weekly_returns: (lookback periods) x assets  -> mean over rows gives Series(index=assets)\n",
    "        mu_ser = w[baseline_key].mean(axis=0)\n",
    "        if not isinstance(mu_ser, pd.Series):\n",
    "            mu_ser = pd.Series(mu_ser, index=w[baseline_key].columns)\n",
    "\n",
    "        # ---- align ----\n",
    "        assets = X_df.index.intersection(y_ser.index).intersection(mu_ser.index)\n",
    "        Xw = X_df.loc[assets].to_numpy(np.float32)\n",
    "        yw = y_ser.loc[assets].to_numpy(np.float32)\n",
    "        muw = mu_ser.loc[assets].to_numpy(np.float32)\n",
    "\n",
    "        # ---- make residual target if requested ----\n",
    "        if residual_over_sample_mean:\n",
    "            yw = yw - muw   # residual = actual - baseline\n",
    "\n",
    "        mask = np.isfinite(Xw).all(axis=1) & np.isfinite(yw) & np.isfinite(muw)\n",
    "        Xw, yw, muw = Xw[mask], yw[mask], muw[mask]\n",
    "        assets_kept = assets.to_numpy()[mask]\n",
    "\n",
    "        X_list.append(Xw)\n",
    "        y_list.append(yw)\n",
    "        mu_list.append(muw)\n",
    "\n",
    "        t0, t1 = w.get(\"t0\", None), w.get(\"t1\", None)\n",
    "        for a in assets_kept:\n",
    "            meta_rows.append((w_idx, a, t0, t1))\n",
    "\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.concatenate(y_list)\n",
    "    mu = np.concatenate(mu_list)\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"window_idx\", \"asset\", \"t0\", \"t1\"])\n",
    "    return X, y, mu, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1497434c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3780, 9),\n",
       " (3780,),\n",
       " (3780,),\n",
       "    window_idx       asset         t0         t1\n",
       " 0           0  dow30:AAPL 2024-11-25 2024-11-27\n",
       " 1           0  dow30:AMGN 2024-11-25 2024-11-27\n",
       " 2           0   dow30:AXP 2024-11-25 2024-11-27\n",
       " 3           0    dow30:BA 2024-11-25 2024-11-27\n",
       " 4           0   dow30:CAT 2024-11-25 2024-11-27)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_past_returns, _, _, _ = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "# X, y, mu, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "X, y, mu, meta = panel_from_windows(rolling, x_key='X_feat')\n",
    "X.shape, y.shape, mu.shape, meta.head() ## len(rolling)*n_asset -- each row is a feature set -- to predict y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "53f316aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2700, 9),\n",
       " (300, 9),\n",
       " (780, 9),\n",
       " (2700,),\n",
       " (300,),\n",
       " (780,),\n",
       " (2700,),\n",
       " (300,),\n",
       " (780,),\n",
       " '26.0 test periods')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = rolling  # your rolling list\n",
    "\n",
    "W = meta[\"window_idx\"].nunique()\n",
    "split_w = int(0.8 * W)\n",
    "\n",
    "train_mask = (meta[\"window_idx\"] < split_w).values\n",
    "test_mask  = (meta[\"window_idx\"] >= split_w).values\n",
    "\n",
    "X_train_raw, y_train = X[train_mask], y[train_mask]\n",
    "X_test_raw,  y_test  = X[test_mask],  y[test_mask]\n",
    "X_past_returns_test_raw = X_past_returns[test_mask]\n",
    "mu_train = mu[train_mask]\n",
    "mu_test  = mu[test_mask]\n",
    "\n",
    "\n",
    "# small validation from the tail of the training windows\n",
    "val_w = max(int(0.1 * split_w), 1)\n",
    "val_start = split_w - val_w\n",
    "val_mask = ((meta[\"window_idx\"] >= val_start) & (meta[\"window_idx\"] < split_w)).values\n",
    "tr2_mask = (meta[\"window_idx\"] < val_start).values\n",
    "\n",
    "X_tr_raw, y_tr = X[tr2_mask], y[tr2_mask]\n",
    "X_va_raw, y_va = X[val_mask], y[val_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr_raw).astype(np.float32)\n",
    "X_va = scaler.transform(X_va_raw).astype(np.float32)\n",
    "X_te = scaler.transform(X_test_raw).astype(np.float32)\n",
    "\n",
    "y_tr = y_tr.astype(np.float32)\n",
    "y_va = y_va.astype(np.float32)\n",
    "y_te = y_test.astype(np.float32)\n",
    "\n",
    "mu_tr = mu[tr2_mask]\n",
    "mu_va = mu[val_mask]\n",
    "\n",
    "X_tr.shape, X_va.shape, X_te.shape, y_tr.shape, y_va.shape, y_te.shape, mu_tr.shape, mu_va.shape, mu_test.shape, f\"{y_te.shape[0]/close_df.shape[1]} test periods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "990892d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.2890 - mae: 0.3948 - val_loss: 0.1817 - val_mae: 0.3174\n",
      "Epoch 2/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2531 - mae: 0.3641 - val_loss: 0.1513 - val_mae: 0.2862\n",
      "Epoch 3/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2216 - mae: 0.3383 - val_loss: 0.1275 - val_mae: 0.2606\n",
      "Epoch 4/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2008 - mae: 0.3193 - val_loss: 0.1090 - val_mae: 0.2391\n",
      "Epoch 5/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1689 - mae: 0.2954 - val_loss: 0.0950 - val_mae: 0.2218\n",
      "Epoch 6/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1522 - mae: 0.2783 - val_loss: 0.0841 - val_mae: 0.2076\n",
      "Epoch 7/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1507 - mae: 0.2708 - val_loss: 0.0750 - val_mae: 0.1954\n",
      "Epoch 8/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1307 - mae: 0.2543 - val_loss: 0.0676 - val_mae: 0.1852\n",
      "Epoch 9/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1172 - mae: 0.2400 - val_loss: 0.0616 - val_mae: 0.1766\n",
      "Epoch 10/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1049 - mae: 0.2272 - val_loss: 0.0564 - val_mae: 0.1690\n",
      "Epoch 11/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1000 - mae: 0.2220 - val_loss: 0.0517 - val_mae: 0.1620\n",
      "Epoch 12/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0945 - mae: 0.2099 - val_loss: 0.0475 - val_mae: 0.1556\n",
      "Epoch 13/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0843 - mae: 0.2002 - val_loss: 0.0439 - val_mae: 0.1497\n",
      "Epoch 14/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0878 - mae: 0.2023 - val_loss: 0.0407 - val_mae: 0.1445\n",
      "Epoch 15/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0732 - mae: 0.1865 - val_loss: 0.0380 - val_mae: 0.1397\n",
      "Epoch 16/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0655 - mae: 0.1819 - val_loss: 0.0356 - val_mae: 0.1353\n",
      "Epoch 17/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0625 - mae: 0.1744 - val_loss: 0.0333 - val_mae: 0.1311\n",
      "Epoch 18/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0613 - mae: 0.1713 - val_loss: 0.0312 - val_mae: 0.1271\n",
      "Epoch 19/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0561 - mae: 0.1672 - val_loss: 0.0293 - val_mae: 0.1234\n",
      "Epoch 20/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0578 - mae: 0.1634 - val_loss: 0.0275 - val_mae: 0.1200\n",
      "Epoch 21/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0522 - mae: 0.1591 - val_loss: 0.0259 - val_mae: 0.1169\n",
      "Epoch 22/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0469 - mae: 0.1493 - val_loss: 0.0245 - val_mae: 0.1140\n",
      "Epoch 23/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0481 - mae: 0.1513 - val_loss: 0.0231 - val_mae: 0.1109\n",
      "Epoch 24/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0429 - mae: 0.1461 - val_loss: 0.0218 - val_mae: 0.1079\n",
      "Epoch 25/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.1447 - val_loss: 0.0207 - val_mae: 0.1052\n",
      "Epoch 26/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0421 - mae: 0.1400 - val_loss: 0.0195 - val_mae: 0.1024\n",
      "Epoch 27/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0380 - mae: 0.1348 - val_loss: 0.0185 - val_mae: 0.0998\n",
      "Epoch 28/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0389 - mae: 0.1347 - val_loss: 0.0175 - val_mae: 0.0974\n",
      "Epoch 29/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0379 - mae: 0.1326 - val_loss: 0.0165 - val_mae: 0.0947\n",
      "Epoch 30/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0370 - mae: 0.1299 - val_loss: 0.0157 - val_mae: 0.0921\n",
      "Epoch 31/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0337 - mae: 0.1261 - val_loss: 0.0148 - val_mae: 0.0893\n",
      "Epoch 32/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0318 - mae: 0.1215 - val_loss: 0.0140 - val_mae: 0.0867\n",
      "Epoch 33/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0295 - mae: 0.1182 - val_loss: 0.0133 - val_mae: 0.0844\n",
      "Epoch 34/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0299 - mae: 0.1156 - val_loss: 0.0126 - val_mae: 0.0823\n",
      "Epoch 35/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0267 - mae: 0.1118 - val_loss: 0.0120 - val_mae: 0.0801\n",
      "Epoch 36/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0273 - mae: 0.1134 - val_loss: 0.0114 - val_mae: 0.0781\n",
      "Epoch 37/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0265 - mae: 0.1083 - val_loss: 0.0109 - val_mae: 0.0765\n",
      "Epoch 38/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0254 - mae: 0.1065 - val_loss: 0.0103 - val_mae: 0.0747\n",
      "Epoch 39/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0223 - mae: 0.1025 - val_loss: 0.0098 - val_mae: 0.0730\n",
      "Epoch 40/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0225 - mae: 0.1027 - val_loss: 0.0094 - val_mae: 0.0715\n",
      "Epoch 41/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0215 - mae: 0.0996 - val_loss: 0.0090 - val_mae: 0.0698\n",
      "Epoch 42/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0229 - mae: 0.1014 - val_loss: 0.0086 - val_mae: 0.0683\n",
      "Epoch 43/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0215 - mae: 0.0957 - val_loss: 0.0081 - val_mae: 0.0668\n",
      "Epoch 44/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0199 - mae: 0.0931 - val_loss: 0.0077 - val_mae: 0.0653\n",
      "Epoch 45/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.0917 - val_loss: 0.0074 - val_mae: 0.0639\n",
      "Epoch 46/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0192 - mae: 0.0917 - val_loss: 0.0071 - val_mae: 0.0628\n",
      "Epoch 47/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0175 - mae: 0.0873 - val_loss: 0.0068 - val_mae: 0.0619\n",
      "Epoch 48/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0168 - mae: 0.0860 - val_loss: 0.0066 - val_mae: 0.0611\n",
      "Epoch 49/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0175 - mae: 0.0865 - val_loss: 0.0064 - val_mae: 0.0602\n",
      "Epoch 50/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0160 - mae: 0.0838 - val_loss: 0.0061 - val_mae: 0.0591\n",
      "Epoch 51/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0156 - mae: 0.0817 - val_loss: 0.0058 - val_mae: 0.0579\n",
      "Epoch 52/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0161 - mae: 0.0825 - val_loss: 0.0056 - val_mae: 0.0570\n",
      "Epoch 53/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0144 - mae: 0.0803 - val_loss: 0.0055 - val_mae: 0.0562\n",
      "Epoch 54/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0156 - mae: 0.0804 - val_loss: 0.0053 - val_mae: 0.0552\n",
      "Epoch 55/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0144 - mae: 0.0787 - val_loss: 0.0051 - val_mae: 0.0542\n",
      "Epoch 56/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0158 - mae: 0.0792 - val_loss: 0.0049 - val_mae: 0.0533\n",
      "Epoch 57/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0127 - mae: 0.0737 - val_loss: 0.0048 - val_mae: 0.0524\n",
      "Epoch 58/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0127 - mae: 0.0743 - val_loss: 0.0046 - val_mae: 0.0515\n",
      "Epoch 59/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0126 - mae: 0.0724 - val_loss: 0.0045 - val_mae: 0.0508\n",
      "Epoch 60/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0123 - mae: 0.0722 - val_loss: 0.0043 - val_mae: 0.0499\n",
      "Epoch 61/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0129 - mae: 0.0719 - val_loss: 0.0041 - val_mae: 0.0487\n",
      "Epoch 62/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0109 - mae: 0.0686 - val_loss: 0.0040 - val_mae: 0.0477\n",
      "Epoch 63/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0122 - mae: 0.0706 - val_loss: 0.0038 - val_mae: 0.0468\n",
      "Epoch 64/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0104 - mae: 0.0649 - val_loss: 0.0037 - val_mae: 0.0458\n",
      "Epoch 65/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0097 - mae: 0.0650 - val_loss: 0.0036 - val_mae: 0.0449\n",
      "Epoch 66/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0099 - mae: 0.0644 - val_loss: 0.0034 - val_mae: 0.0438\n",
      "Epoch 67/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0102 - mae: 0.0642 - val_loss: 0.0033 - val_mae: 0.0428\n",
      "Epoch 68/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0099 - mae: 0.0628 - val_loss: 0.0032 - val_mae: 0.0422\n",
      "Epoch 69/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0093 - mae: 0.0611 - val_loss: 0.0031 - val_mae: 0.0415\n",
      "Epoch 70/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0097 - mae: 0.0612 - val_loss: 0.0031 - val_mae: 0.0411\n",
      "Epoch 71/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0084 - mae: 0.0601 - val_loss: 0.0030 - val_mae: 0.0406\n",
      "Epoch 72/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0094 - mae: 0.0604 - val_loss: 0.0030 - val_mae: 0.0403\n",
      "Epoch 73/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0083 - mae: 0.0578 - val_loss: 0.0030 - val_mae: 0.0399\n",
      "Epoch 74/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0086 - mae: 0.0574 - val_loss: 0.0029 - val_mae: 0.0395\n",
      "Epoch 75/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0085 - mae: 0.0572 - val_loss: 0.0029 - val_mae: 0.0390\n",
      "Epoch 76/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0077 - mae: 0.0553 - val_loss: 0.0028 - val_mae: 0.0383\n",
      "Epoch 77/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0086 - mae: 0.0570 - val_loss: 0.0027 - val_mae: 0.0380\n",
      "Epoch 78/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0078 - mae: 0.0552 - val_loss: 0.0027 - val_mae: 0.0376\n",
      "Epoch 79/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0076 - mae: 0.0545 - val_loss: 0.0026 - val_mae: 0.0370\n",
      "Epoch 80/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0072 - mae: 0.0534 - val_loss: 0.0026 - val_mae: 0.0364\n",
      "Epoch 81/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0074 - mae: 0.0516 - val_loss: 0.0025 - val_mae: 0.0358\n",
      "Epoch 82/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0071 - mae: 0.0506 - val_loss: 0.0024 - val_mae: 0.0353\n",
      "Epoch 83/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0067 - mae: 0.0507 - val_loss: 0.0023 - val_mae: 0.0346\n",
      "Epoch 84/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0072 - mae: 0.0513 - val_loss: 0.0023 - val_mae: 0.0341\n",
      "Epoch 85/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0065 - mae: 0.0488 - val_loss: 0.0022 - val_mae: 0.0336\n",
      "Epoch 86/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0062 - mae: 0.0485 - val_loss: 0.0022 - val_mae: 0.0332\n",
      "Epoch 87/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0071 - mae: 0.0488 - val_loss: 0.0021 - val_mae: 0.0328\n",
      "Epoch 88/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0057 - mae: 0.0474 - val_loss: 0.0021 - val_mae: 0.0323\n",
      "Epoch 89/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0067 - mae: 0.0477 - val_loss: 0.0020 - val_mae: 0.0320\n",
      "Epoch 90/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0064 - mae: 0.0466 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 91/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0059 - mae: 0.0465 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 92/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0055 - mae: 0.0459 - val_loss: 0.0019 - val_mae: 0.0311\n",
      "Epoch 93/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0053 - mae: 0.0439 - val_loss: 0.0019 - val_mae: 0.0306\n",
      "Epoch 94/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0069 - mae: 0.0456 - val_loss: 0.0018 - val_mae: 0.0300\n",
      "Epoch 95/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0048 - mae: 0.0430 - val_loss: 0.0018 - val_mae: 0.0296\n",
      "Epoch 96/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0048 - mae: 0.0421 - val_loss: 0.0017 - val_mae: 0.0291\n",
      "Epoch 97/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0051 - mae: 0.0420 - val_loss: 0.0017 - val_mae: 0.0285\n",
      "Epoch 98/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0056 - mae: 0.0428 - val_loss: 0.0016 - val_mae: 0.0282\n",
      "Epoch 99/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 - mae: 0.0427 - val_loss: 0.0016 - val_mae: 0.0277\n",
      "Epoch 100/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0054 - mae: 0.0428 - val_loss: 0.0016 - val_mae: 0.0275\n",
      "Epoch 101/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0048 - mae: 0.0414 - val_loss: 0.0015 - val_mae: 0.0272\n",
      "Epoch 102/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0046 - mae: 0.0396 - val_loss: 0.0015 - val_mae: 0.0270\n",
      "Epoch 103/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0050 - mae: 0.0403 - val_loss: 0.0015 - val_mae: 0.0268\n",
      "Epoch 104/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0404 - val_loss: 0.0015 - val_mae: 0.0266\n",
      "Epoch 105/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0043 - mae: 0.0392 - val_loss: 0.0014 - val_mae: 0.0262\n",
      "Epoch 106/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - mae: 0.0377 - val_loss: 0.0014 - val_mae: 0.0259\n",
      "Epoch 107/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0042 - mae: 0.0379 - val_loss: 0.0014 - val_mae: 0.0256\n",
      "Epoch 108/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0045 - mae: 0.0383 - val_loss: 0.0013 - val_mae: 0.0253\n",
      "Epoch 109/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - mae: 0.0373 - val_loss: 0.0013 - val_mae: 0.0249\n",
      "Epoch 110/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0044 - mae: 0.0377 - val_loss: 0.0013 - val_mae: 0.0246\n",
      "Epoch 111/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - mae: 0.0379 - val_loss: 0.0012 - val_mae: 0.0243\n",
      "Epoch 112/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 - mae: 0.0382 - val_loss: 0.0012 - val_mae: 0.0240\n",
      "Epoch 113/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - mae: 0.0358 - val_loss: 0.0012 - val_mae: 0.0238\n",
      "Epoch 114/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - mae: 0.0357 - val_loss: 0.0012 - val_mae: 0.0237\n",
      "Epoch 115/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - mae: 0.0361 - val_loss: 0.0011 - val_mae: 0.0234\n",
      "Epoch 116/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0035 - mae: 0.0354 - val_loss: 0.0011 - val_mae: 0.0231\n",
      "Epoch 117/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - mae: 0.0354 - val_loss: 0.0011 - val_mae: 0.0228\n",
      "Epoch 118/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - mae: 0.0346 - val_loss: 0.0011 - val_mae: 0.0226\n",
      "Epoch 119/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - mae: 0.0353 - val_loss: 0.0011 - val_mae: 0.0224\n",
      "Epoch 120/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0034 - mae: 0.0340 - val_loss: 0.0010 - val_mae: 0.0221\n",
      "Epoch 121/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0035 - mae: 0.0336 - val_loss: 0.0010 - val_mae: 0.0219\n",
      "Epoch 122/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - mae: 0.0331 - val_loss: 9.9600e-04 - val_mae: 0.0217\n",
      "Epoch 123/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028 - mae: 0.0320 - val_loss: 9.7551e-04 - val_mae: 0.0215\n",
      "Epoch 124/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - mae: 0.0336 - val_loss: 9.5482e-04 - val_mae: 0.0212\n",
      "Epoch 125/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030 - mae: 0.0319 - val_loss: 9.3621e-04 - val_mae: 0.0210\n",
      "Epoch 126/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - mae: 0.0328 - val_loss: 9.1867e-04 - val_mae: 0.0208\n",
      "Epoch 127/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028 - mae: 0.0314 - val_loss: 9.0533e-04 - val_mae: 0.0206\n",
      "Epoch 128/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - mae: 0.0320 - val_loss: 8.9014e-04 - val_mae: 0.0204\n",
      "Epoch 129/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0029 - mae: 0.0308 - val_loss: 8.7278e-04 - val_mae: 0.0202\n",
      "Epoch 130/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033 - mae: 0.0316 - val_loss: 8.5978e-04 - val_mae: 0.0201\n",
      "Epoch 131/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028 - mae: 0.0307 - val_loss: 8.4420e-04 - val_mae: 0.0198\n",
      "Epoch 132/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0030 - mae: 0.0311 - val_loss: 8.3021e-04 - val_mae: 0.0197\n",
      "Epoch 133/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023 - mae: 0.0288 - val_loss: 8.1688e-04 - val_mae: 0.0195\n",
      "Epoch 134/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030 - mae: 0.0304 - val_loss: 7.9475e-04 - val_mae: 0.0192\n",
      "Epoch 135/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0029 - mae: 0.0305 - val_loss: 7.7857e-04 - val_mae: 0.0190\n",
      "Epoch 136/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028 - mae: 0.0296 - val_loss: 7.6378e-04 - val_mae: 0.0188\n",
      "Epoch 137/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0023 - mae: 0.0280 - val_loss: 7.5043e-04 - val_mae: 0.0187\n",
      "Epoch 138/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025 - mae: 0.0287 - val_loss: 7.3938e-04 - val_mae: 0.0185\n",
      "Epoch 139/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0026 - mae: 0.0288 - val_loss: 7.2863e-04 - val_mae: 0.0184\n",
      "Epoch 140/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028 - mae: 0.0294 - val_loss: 7.1793e-04 - val_mae: 0.0183\n",
      "Epoch 141/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028 - mae: 0.0292 - val_loss: 7.0089e-04 - val_mae: 0.0180\n",
      "Epoch 142/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0023 - mae: 0.0281 - val_loss: 6.8855e-04 - val_mae: 0.0179\n",
      "Epoch 143/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0019 - mae: 0.0269 - val_loss: 6.7714e-04 - val_mae: 0.0178\n",
      "Epoch 144/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0026 - mae: 0.0288 - val_loss: 6.6594e-04 - val_mae: 0.0176\n",
      "Epoch 145/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - mae: 0.0280 - val_loss: 6.5507e-04 - val_mae: 0.0175\n",
      "Epoch 146/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0027 - mae: 0.0290 - val_loss: 6.4868e-04 - val_mae: 0.0174\n",
      "Epoch 147/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0029 - mae: 0.0283 - val_loss: 6.4103e-04 - val_mae: 0.0173\n",
      "Epoch 148/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024 - mae: 0.0281 - val_loss: 6.3358e-04 - val_mae: 0.0173\n",
      "Epoch 149/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0020 - mae: 0.0267 - val_loss: 6.2613e-04 - val_mae: 0.0172\n",
      "Epoch 150/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mae: 0.0275 - val_loss: 6.1636e-04 - val_mae: 0.0170\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(in_dim, hidden=(64, 32), dropout=0.1, lr=1e-3):\n",
    "    inputs = keras.Input(shape=(in_dim,))\n",
    "    x = inputs\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_mlp(in_dim=X_tr.shape[1], hidden=(16, 8), dropout=0.1, lr=3e-4)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=20, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_va, y_va),\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "0516900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def prediction_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    sign_acc = (np.sign(y_true) == np.sign(y_pred)).mean()\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan\n",
    "    \n",
    "    return {\"MSE\": round(mse, 5), \"MAE\": round(mae, 5), \"R2\": round(r2, 5), \"SignAcc\": round(float(sign_acc), 5), \"Corr\": round(float(corr), 5)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b67e147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean\n",
      "{'MSE': 0.00039, 'MAE': 0.01341, 'R2': -0.06373, 'SignAcc': 0.49359, 'Corr': -0.025}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "MLP\n",
      "{'MSE': 0.00078, 'MAE': 0.01902, 'R2': -1.15339, 'SignAcc': 0.47564, 'Corr': 0.00916}\n",
      "-0.00031272165\n",
      "0.0011933894\n",
      "0.004373363\n"
     ]
    }
   ],
   "source": [
    "return_true_test = y_te + mu_test\n",
    "\n",
    "y_pred_sm = mu_test\n",
    "print(\"Sample Mean\")\n",
    "print(prediction_metrics(return_true_test, y_pred_sm))\n",
    "\n",
    "y_pred_residual = model.predict(X_te, batch_size=1024).squeeze()\n",
    "return_pred = mu_test + y_pred_residual\n",
    "print(\"MLP\")\n",
    "print(prediction_metrics(return_true_test, return_pred))\n",
    "\n",
    "print(y_te.mean())\n",
    "print(return_true_test.mean())\n",
    "print(y_pred_residual.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e5e0cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true_test quantiles: [-0.08066442 -0.04359215  0.000701    0.05251097  0.20515133]\n",
      "mu_test quantiles    : [-0.01007372 -0.00743506  0.00090595  0.01633628  0.02404095]\n"
     ]
    }
   ],
   "source": [
    "y_true_test = y_te + mu_test\n",
    "print(\"y_true_test quantiles:\", np.quantile(y_true_test, [0, .01, .5, .99, 1]))\n",
    "print(\"mu_test quantiles    :\", np.quantile(mu_test, [0, .01, .5, .99, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce8890",
   "metadata": {},
   "source": [
    "Walk-Forward For Residual Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2ed0db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_sample_mean(test_window, baseline_key=\"past_weekly_returns\"):\n",
    "    # returns Series indexed by asset\n",
    "    return test_window[baseline_key].mean(axis=0)\n",
    "\n",
    "def walk_forward_eval_mlp_residual(\n",
    "    rolling,\n",
    "    train_len=150,\n",
    "    x_key=\"X_feat\",\n",
    "    y_key=\"y_ret\",\n",
    "    baseline_key=\"past_weekly_returns\",   # where to compute mu from\n",
    "    hidden=(64, 32),\n",
    "    dropout=0.1,\n",
    "    lr=3e-4,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    seed=42\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    all_rows = []\n",
    "    period_metrics = []\n",
    "\n",
    "    for i in range(train_len, len(rolling)):\n",
    "        train_windows = rolling[i-train_len:i]\n",
    "        test_window   = rolling[i]\n",
    "\n",
    "        # --- TRAIN panel: residual target ---\n",
    "        X_train, y_res_train, mu_train, meta_train = panel_from_windows(\n",
    "            train_windows,\n",
    "            x_key=x_key,\n",
    "            y_key=y_key,\n",
    "            residual_over_sample_mean=True,\n",
    "            baseline_key=baseline_key,\n",
    "        )\n",
    "        if X_train.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # --- scale X on TRAIN only ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "        # --- build TEST cross-section ---\n",
    "        X_test_df = test_window[x_key]  # assets x features\n",
    "        if x_key == 'past_weekly_returns':\n",
    "            X_test_df = X_test_df.T\n",
    "        y_test = test_window[y_key]\n",
    "        if not isinstance(y_test, pd.Series):\n",
    "            y_test = pd.Series(y_test, index=X_test_df.index)\n",
    "\n",
    "        mu_ser = baseline_sample_mean(test_window, baseline_key=baseline_key)\n",
    "\n",
    "        # align assets for X, y_true, mu\n",
    "        assets = X_test_df.index.intersection(y_test.index).intersection(mu_ser.index).sort_values()\n",
    "\n",
    "        X_test = X_test_df.loc[assets].to_numpy(np.float32)\n",
    "        y_true = y_test.loc[assets].to_numpy(np.float32)      # TRUE return\n",
    "        mu_test = mu_ser.loc[assets].to_numpy(np.float32)     # baseline return forecast\n",
    "\n",
    "        mask = np.isfinite(X_test).all(axis=1) & np.isfinite(y_true) & np.isfinite(mu_test)\n",
    "        assets = assets[mask]\n",
    "        X_test = X_test[mask]\n",
    "        y_true = y_true[mask]\n",
    "        mu_test = mu_test[mask]\n",
    "\n",
    "        X_test_sc = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "        # --- train fresh model each step ---\n",
    "        model = build_mlp(in_dim=X_train_sc.shape[1], hidden=hidden, dropout=dropout, lr=lr)\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_sc, y_res_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "\n",
    "        # --- predict residual, reconstruct return ---\n",
    "        pred_res = model.predict(X_test_sc, batch_size=1024, verbose=0).squeeze()\n",
    "        y_pred_return = mu_test + pred_res\n",
    "\n",
    "        # --- baseline prediction (sample mean) ---\n",
    "        y_pred_sm = mu_test\n",
    "\n",
    "        # store per-asset predictions\n",
    "        for a, yt, yhat, yhat_sm, muhat, rhat in zip(assets, y_true, y_pred_return, y_pred_sm, mu_test, pred_res):\n",
    "            all_rows.append({\n",
    "                \"window_idx\": i,\n",
    "                \"t0\": test_window.get(\"t0\", None),\n",
    "                \"t1\": test_window.get(\"t1\", None),\n",
    "                \"asset\": a,\n",
    "                \"y_true\": float(yt),\n",
    "                \"mu\": float(muhat),\n",
    "                \"pred_res\": float(rhat),\n",
    "                \"pred_mlp\": float(yhat),\n",
    "                \"pred_sm\": float(yhat_sm),\n",
    "            })\n",
    "\n",
    "        # per-window metrics (cross-section) on TRUE returns\n",
    "        m_mlp = prediction_metrics(y_true, y_pred_return)\n",
    "        m_sm  = prediction_metrics(y_true, y_pred_sm)\n",
    "\n",
    "        period_metrics.append({\n",
    "            \"window_idx\": i,\n",
    "            \"t0\": test_window.get(\"t0\", None),\n",
    "            \"t1\": test_window.get(\"t1\", None),\n",
    "            **{f\"mlp_{k}\": v for k, v in m_mlp.items()},\n",
    "            **{f\"sm_{k}\": v for k, v in m_sm.items()},\n",
    "        })\n",
    "        \n",
    "        print(\"X_train raw min/max:\", X_train.min(), X_train.max())\n",
    "        print(\"y_res_train min/max:\", y_res_train.min(), y_res_train.max())\n",
    "        print(\"mu_train min/max:\", mu_train.min(), mu_train.max())\n",
    "    \n",
    "    preds_df = pd.DataFrame(all_rows)\n",
    "    period_df = pd.DataFrame(period_metrics)\n",
    "\n",
    "    pooled_mlp = prediction_metrics(preds_df[\"y_true\"], preds_df[\"pred_mlp\"]) if len(preds_df) else {}\n",
    "    pooled_sm  = prediction_metrics(preds_df[\"y_true\"], preds_df[\"pred_sm\"])  if len(preds_df) else {}\n",
    "    \n",
    "\n",
    "    return preds_df, period_df, pooled_mlp, pooled_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "4e8e7c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train raw min/max: -5.2934895 5.294527\n",
      "y_res_train min/max: -0.18489625 0.13687612\n",
      "mu_train min/max: -0.02251788 0.014512167\n",
      "X_train raw min/max: -5.2934895 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.014512167\n",
      "X_train raw min/max: -5.2934895 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.014512167\n",
      "X_train raw min/max: -5.2934895 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.014512167\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.014512167\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.015215398\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.015215398\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.015215398\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.015215398\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.01668494\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.01668494\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.01668494\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.01668494\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.020368239\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.020368239\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.022810148\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "X_train raw min/max: -5.292356 5.294527\n",
      "y_res_train min/max: -0.18489625 0.19928172\n",
      "mu_train min/max: -0.02251788 0.024040952\n",
      "Pooled MLP: {'MSE': 0.00132, 'MAE': 0.02314, 'R2': -2.64363, 'SignAcc': 0.47692, 'Corr': -0.00971}\n",
      "Pooled  SM: {'MSE': 0.00039, 'MAE': 0.01341, 'R2': -0.06373, 'SignAcc': 0.49359, 'Corr': -0.025}\n",
      "Win-rate (MAE): 0.038461538461538464\n"
     ]
    }
   ],
   "source": [
    "preds_df, period_df, pooled_mlp, pooled_sm = walk_forward_eval_mlp_residual(\n",
    "    rolling=rolling,\n",
    "    train_len=100,\n",
    "    x_key=\"X_feat\",\n",
    "    # x_key=\"past_weekly_returns\",\n",
    "    y_key=\"y_ret\",\n",
    "    baseline_key=\"past_weekly_returns\",\n",
    "    hidden=(16,8)\n",
    ")\n",
    "\n",
    "print(\"Pooled MLP:\", pooled_mlp)\n",
    "print(\"Pooled  SM:\", pooled_sm)\n",
    "print(\"Win-rate (MAE):\", (period_df[\"mlp_MAE\"] < period_df[\"sm_MAE\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "450c1f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train raw min/max: -5.383983 5.38163\n",
      "y_res_train min/max: -0.18489625 0.13687612\n",
      "mu_train min/max: -0.02251788 0.014512167\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train raw min/max:\", X_tr.min(), X_tr.max())\n",
    "print(\"y_res_train min/max:\", y_tr.min(), y_tr.max())\n",
    "print(\"mu_train min/max:\", mu_tr.min(), mu_tr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685538b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
