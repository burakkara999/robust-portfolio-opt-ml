{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520d7e1f",
   "metadata": {},
   "source": [
    "# LSTM Return Prediction (2-day horizon)\n",
    "\n",
    "This notebook mirrors the **MLP** notebook structure, but replaces the predictor with an **LSTM**.\n",
    "\n",
    "- We build rolling windows from close prices (same `make_feature_windows` pipeline).\n",
    "- Each **sample = one (window, asset)** pair.\n",
    "- Input = the past sequence of returns (a 1D sequence), reshaped to `(timesteps, 1)`.\n",
    "- Target = next-period return (configured so that **1 period = 2 trading days** via `days_per_week=2`).\n",
    "\n",
    "The LSTM hyperparameter ranges and the “best” configuration are taken from the Ma et al. (2021) paper (Table 2 / Section 3.2).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e027dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# project utilities (same as your MLP notebook)\n",
    "from asset_data_module import read_close_prices_all_merged\n",
    "from features import make_feature_windows\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22d5b5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((981, 30), 430)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1) Load data + build rolling windows\n",
    "# ----------------------------\n",
    "markets = ['dow30']\n",
    "# markets = ['commodities']   # <-- change as needed\n",
    "\n",
    "start_date, end_date = \"2022-01-01\", \"2025-11-28\"\n",
    "\n",
    "_, close_df = read_close_prices_all_merged(markets, after_date=start_date)\n",
    "close_df = close_df.loc[:end_date]\n",
    "\n",
    "# IMPORTANT:\n",
    "# days_per_week=2 means \"1 period\" corresponds to 2 trading days in your window builder.\n",
    "# So horizon=1 => predicting the return over the next 2 trading days.\n",
    "rolling = make_feature_windows(\n",
    "    close_prices=close_df,\n",
    "    lookback=60,\n",
    "    horizon=1,\n",
    "    days_per_week=2\n",
    ")\n",
    "\n",
    "close_df.shape, len(rolling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ccb755f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12900, 60),\n",
       " (12900,),\n",
       "    window_idx       asset         t0         t1\n",
       " 0           0  dow30:AAPL 2022-06-24 2022-06-28\n",
       " 1           0  dow30:AMGN 2022-06-24 2022-06-28\n",
       " 2           0   dow30:AXP 2022-06-24 2022-06-28\n",
       " 3           0    dow30:BA 2022-06-24 2022-06-28\n",
       " 4           0   dow30:CAT 2022-06-24 2022-06-28)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2) Build panel dataset: (window, asset) -> (sequence, target)\n",
    "# ----------------------------\n",
    "def panel_from_windows(windows, x_key=\"past_weekly_returns\", y_key=\"y_ret\"):\n",
    "    X_list, y_list = [], []\n",
    "    meta_rows = []\n",
    "\n",
    "    for w_idx, w in enumerate(windows):\n",
    "        if x_key == 'past_weekly_returns':\n",
    "            X_df = w[x_key].T          # assets x n_lookback (DataFrame)\n",
    "        elif x_key == 'X_feat':\n",
    "            X_df = w[x_key]            # assets x n_features (DataFrame)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown x_key: {x_key}\")\n",
    "\n",
    "        y_ser = w[y_key]\n",
    "        if not isinstance(y_ser, pd.Series):\n",
    "            y_ser = pd.Series(y_ser, index=X_df.index)\n",
    "\n",
    "        assets = X_df.index.intersection(y_ser.index)\n",
    "        Xw = X_df.loc[assets].to_numpy(dtype=np.float32)\n",
    "        yw = y_ser.loc[assets].to_numpy(dtype=np.float32)\n",
    "\n",
    "        mask = np.isfinite(Xw).all(axis=1) & np.isfinite(yw)\n",
    "        Xw, yw = Xw[mask], yw[mask]\n",
    "        assets_kept = assets.to_numpy()[mask]\n",
    "\n",
    "        X_list.append(Xw)\n",
    "        y_list.append(yw)\n",
    "\n",
    "        t0, t1 = w.get(\"t0\", None), w.get(\"t1\", None)\n",
    "        for a in assets_kept:\n",
    "            meta_rows.append((w_idx, a, t0, t1))\n",
    "\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.concatenate(y_list)\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"window_idx\", \"asset\", \"t0\", \"t1\"])\n",
    "    return X, y, meta\n",
    "\n",
    "# sequences: shape (samples, n_lookback)\n",
    "X_seq_raw, y_raw, meta = panel_from_windows(rolling, x_key=\"past_weekly_returns\", y_key=\"y_ret\")\n",
    "\n",
    "X_seq_raw.shape, y_raw.shape, meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9bc3414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test shapes (raw): (9300, 60) (1020, 60) (2580, 60) (9300,) (1020,) (2580,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((9300, 60, 1), (9300,))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3) Train/val/test split by time (window index) + scaling\n",
    "# ----------------------------\n",
    "W = meta[\"window_idx\"].nunique()\n",
    "split_w = int(0.8 * W)\n",
    "\n",
    "train_mask = (meta[\"window_idx\"] < split_w).values\n",
    "test_mask  = (meta[\"window_idx\"] >= split_w).values\n",
    "\n",
    "X_train_raw, y_train_raw = X_seq_raw[train_mask], y_raw[train_mask]\n",
    "X_test_raw,  y_test_raw  = X_seq_raw[test_mask],  y_raw[test_mask]\n",
    "\n",
    "# validation: last 10% of training windows\n",
    "val_w = max(int(0.1 * split_w), 1)\n",
    "val_start = split_w - val_w\n",
    "val_mask = ((meta[\"window_idx\"] >= val_start) & (meta[\"window_idx\"] < split_w)).values\n",
    "\n",
    "X_val_raw, y_val_raw = X_seq_raw[val_mask], y_raw[val_mask]\n",
    "\n",
    "# remove val from train\n",
    "train_mask2 = train_mask & (~val_mask)\n",
    "X_train_raw, y_train_raw = X_seq_raw[train_mask2], y_raw[train_mask2]\n",
    "\n",
    "print(\"Train/Val/Test shapes (raw):\",\n",
    "      X_train_raw.shape, X_val_raw.shape, X_test_raw.shape,\n",
    "      y_train_raw.shape, y_val_raw.shape, y_test_raw.shape)\n",
    "\n",
    "# --- scale X (fit on TRAIN only) ---\n",
    "# For sequences, fit scaler on all training timesteps pooled together.\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(X_train_raw.reshape(-1, 1))\n",
    "\n",
    "def scale_seq(X):\n",
    "    Xs = scaler_x.transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "    return Xs.astype(np.float32)\n",
    "\n",
    "X_train = scale_seq(X_train_raw)\n",
    "X_val   = scale_seq(X_val_raw)\n",
    "X_test  = scale_seq(X_test_raw)\n",
    "\n",
    "# reshape to (samples, timesteps, features=1) for LSTM\n",
    "X_train = X_train[..., None]\n",
    "X_val   = X_val[..., None]\n",
    "X_test  = X_test[..., None]\n",
    "\n",
    "# --- scale y (optional but usually helps) ---\n",
    "y_mean = y_train_raw.mean()\n",
    "y_std  = y_train_raw.std() + 1e-8\n",
    "\n",
    "def scale_y(y):\n",
    "    return ((y - y_mean) / y_std).astype(np.float32)\n",
    "\n",
    "def unscale_y(y_s):\n",
    "    return (y_s * y_std + y_mean).astype(np.float32)\n",
    "\n",
    "y_train = y_train_raw\n",
    "y_val   = y_val_raw\n",
    "y_test  = y_test_raw.astype(np.float32)\n",
    "\n",
    "# y_train = scale_y(y_train_raw)\n",
    "# y_val   = scale_y(y_val_raw)\n",
    "# y_test  = y_test_raw.astype(np.float32)\n",
    "\n",
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f31ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m5\u001b[0m)          │           \u001b[38;5;34m140\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m220\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">366</span> (1.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m366\u001b[0m (1.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">366</span> (1.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m366\u001b[0m (1.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4) LSTM model (paper-style)\n",
    "# ----------------------------\n",
    "def build_lstm(\n",
    "    timesteps,\n",
    "    units=5,\n",
    "    n_layers=4,\n",
    "    dropout=0.4,\n",
    "    recurrent_dropout=0.3,\n",
    "    activation=\"relu\",\n",
    "    lr=0.01,\n",
    "    optimizer_name=\"RMSprop\",\n",
    "    loss=\"mae\",\n",
    "):\n",
    "    inp = keras.Input(shape=(timesteps, 1))\n",
    "    x = inp\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        return_seq = (i < n_layers - 1)\n",
    "        x = layers.LSTM(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            return_sequences=return_seq,\n",
    "        )(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"linear\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "\n",
    "    opt_name = optimizer_name.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    elif opt_name == \"adam\":\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif opt_name == \"sgd\":\n",
    "        opt = keras.optimizers.SGD(learning_rate=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "# paper's reported \"best\" LSTM config:\n",
    "# 4 layers, 5 nodes each, lr=0.01, batch=100, dropout=0.4, rec_dropout=0.3, optimizer=RMSprop, loss=MAE\n",
    "timesteps = X_train.shape[1]\n",
    "# model = build_lstm(\n",
    "#     timesteps=timesteps,\n",
    "#     units=5,\n",
    "#     n_layers=4,\n",
    "#     dropout=0.4,\n",
    "#     recurrent_dropout=0.3,\n",
    "#     activation=\"relu\",\n",
    "#     lr=0.01,\n",
    "#     # optimizer_name=\"RMSprop\",\n",
    "#     optimizer_name=\"adam\",\n",
    "#     loss=\"mae\",\n",
    "# )\n",
    "\n",
    "model = build_lstm(\n",
    "    timesteps=timesteps,\n",
    "    units=5,\n",
    "    n_layers=2,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2,\n",
    "    activation=\"relu\",\n",
    "    # activation=\"tanh\",\n",
    "    lr=0.01,\n",
    "    # optimizer_name=\"RMSprop\",\n",
    "    optimizer_name=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8caa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0199 - mae: 0.0199 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 2/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0117 - mae: 0.0117 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 3/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0113 - mae: 0.0113 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 4/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 5/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 6/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0111 - mae: 0.0111 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 7/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 8/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 9/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 10/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 11/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0117 - val_mae: 0.0117\n",
      "Epoch 12/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 13/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 14/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 15/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 16/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 17/20\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0110 - mae: 0.0110 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 18/20\n",
      "\u001b[1m11/93\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0106 - mae: 0.0106"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5) Train\n",
    "# ----------------------------\n",
    "# epochs = 200\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "patience = 0  # paper lists \"patient=0\" (we interpret this as \"no early stopping\")\n",
    "\n",
    "callbacks = []\n",
    "if patience and patience > 0:\n",
    "    callbacks.append(\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead34cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM</th>\n",
       "      <th>SampleMean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.013677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>-0.004991</td>\n",
       "      <td>-0.053229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DirAcc</th>\n",
       "      <td>0.545349</td>\n",
       "      <td>0.482946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LSTM  SampleMean\n",
       "MSE     0.000466    0.000488\n",
       "MAE     0.013221    0.013677\n",
       "R2     -0.004991   -0.053229\n",
       "DirAcc  0.545349    0.482946"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 6) Evaluate vs baseline (sample mean)\n",
    "# ----------------------------\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# LSTM predictions (scaled -> unscaled)\n",
    "y_pred_s = model.predict(X_test, batch_size=1024, verbose=0).squeeze()\n",
    "y_pred = unscale_y(y_pred_s)\n",
    "\n",
    "# baseline: sample mean of past sequence (unscaled)\n",
    "y_pred_sm = X_test_raw.mean(axis=1).astype(np.float32)\n",
    "\n",
    "def dir_acc(y_true, y_hat):\n",
    "    return float((np.sign(y_true) == np.sign(y_hat)).mean())\n",
    "\n",
    "metrics_lstm = {\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "    \"R2\":  r2_score(y_test, y_pred),\n",
    "    \"DirAcc\": dir_acc(y_test, y_pred),\n",
    "}\n",
    "\n",
    "metrics_sm = {\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_sm),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_sm),\n",
    "    \"R2\":  r2_score(y_test, y_pred_sm),\n",
    "    \"DirAcc\": dir_acc(y_test, y_pred_sm),\n",
    "}\n",
    "\n",
    "pd.DataFrame({\"LSTM\": metrics_lstm, \"SampleMean\": metrics_sm})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a40fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.011637250892817974 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.2, 'recurrent_dropout': 0.2, 'optimizer_name': 'RMSprop', 'activation': 'relu', 'loss': 'mae'}\n",
      "1 0.01163478847593069 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.2, 'recurrent_dropout': 0.2, 'optimizer_name': 'Adam', 'activation': 'relu', 'loss': 'mae'}\n",
      "2 0.011637943796813488 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.2, 'recurrent_dropout': 0.3, 'optimizer_name': 'RMSprop', 'activation': 'relu', 'loss': 'mae'}\n",
      "3 0.011640357784926891 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.2, 'recurrent_dropout': 0.3, 'optimizer_name': 'Adam', 'activation': 'relu', 'loss': 'mae'}\n",
      "4 0.011633374728262424 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.4, 'recurrent_dropout': 0.2, 'optimizer_name': 'RMSprop', 'activation': 'relu', 'loss': 'mae'}\n",
      "5 0.011636395007371902 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.4, 'recurrent_dropout': 0.2, 'optimizer_name': 'Adam', 'activation': 'relu', 'loss': 'mae'}\n",
      "6 0.011634252965450287 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.4, 'recurrent_dropout': 0.3, 'optimizer_name': 'RMSprop', 'activation': 'relu', 'loss': 'mae'}\n",
      "7 0.011635263450443745 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 50, 'dropout': 0.4, 'recurrent_dropout': 0.3, 'optimizer_name': 'Adam', 'activation': 'relu', 'loss': 'mae'}\n",
      "8 0.011637736111879349 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 100, 'dropout': 0.2, 'recurrent_dropout': 0.2, 'optimizer_name': 'RMSprop', 'activation': 'relu', 'loss': 'mae'}\n",
      "9 0.01164238154888153 {'units': 5, 'n_layers': 1, 'lr': 0.001, 'batch_size': 100, 'dropout': 0.2, 'recurrent_dropout': 0.2, 'optimizer_name': 'Adam', 'activation': 'relu', 'loss': 'mae'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>recurrent_dropout</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>activation</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_MAE</th>\n",
       "      <th>val_MSE</th>\n",
       "      <th>val_DirAcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011634</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011637</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>relu</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.011642</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.503922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  n_layers     lr  batch_size  dropout  recurrent_dropout  \\\n",
       "4      5         1  0.001          50      0.4                0.2   \n",
       "6      5         1  0.001          50      0.4                0.3   \n",
       "1      5         1  0.001          50      0.2                0.2   \n",
       "7      5         1  0.001          50      0.4                0.3   \n",
       "5      5         1  0.001          50      0.4                0.2   \n",
       "0      5         1  0.001          50      0.2                0.2   \n",
       "8      5         1  0.001         100      0.2                0.2   \n",
       "2      5         1  0.001          50      0.2                0.3   \n",
       "3      5         1  0.001          50      0.2                0.3   \n",
       "9      5         1  0.001         100      0.2                0.2   \n",
       "\n",
       "  optimizer_name activation loss   val_MAE  val_MSE  val_DirAcc  \n",
       "4        RMSprop       relu  mae  0.011633  0.00029    0.503922  \n",
       "6        RMSprop       relu  mae  0.011634  0.00029    0.503922  \n",
       "1           Adam       relu  mae  0.011635  0.00029    0.503922  \n",
       "7           Adam       relu  mae  0.011635  0.00029    0.503922  \n",
       "5           Adam       relu  mae  0.011636  0.00029    0.503922  \n",
       "0        RMSprop       relu  mae  0.011637  0.00029    0.503922  \n",
       "8        RMSprop       relu  mae  0.011638  0.00029    0.503922  \n",
       "2        RMSprop       relu  mae  0.011638  0.00029    0.503922  \n",
       "3           Adam       relu  mae  0.011640  0.00029    0.503922  \n",
       "9           Adam       relu  mae  0.011642  0.00029    0.503922  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 7) (Optional) Small grid search scaffold (paper lists large ranges in Table 2)\n",
    "# ----------------------------\n",
    "from itertools import product\n",
    "\n",
    "grid = {\n",
    "    \"units\": [5, 10, 15],\n",
    "    \"n_layers\": [1, 2, 4],\n",
    "    \"lr\": [1e-3, 1e-2],\n",
    "    \"batch_size\": [50, 100],\n",
    "    \"dropout\": [0.2, 0.4],\n",
    "    \"recurrent_dropout\": [0.2, 0.3],\n",
    "    \"optimizer_name\": [\"RMSprop\", \"Adam\"],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"loss\": [\"mae\"],\n",
    "}\n",
    "\n",
    "def train_eval_one(cfg, epochs=100, patience=5):\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    m = build_lstm(\n",
    "        timesteps=X_train.shape[1],\n",
    "        units=cfg[\"units\"],\n",
    "        n_layers=cfg[\"n_layers\"],\n",
    "        dropout=cfg[\"dropout\"],\n",
    "        recurrent_dropout=cfg[\"recurrent_dropout\"],\n",
    "        activation=cfg[\"activation\"],\n",
    "        lr=cfg[\"lr\"],\n",
    "        optimizer_name=cfg[\"optimizer_name\"],\n",
    "        loss=cfg[\"loss\"],\n",
    "    )\n",
    "\n",
    "    callbacks = []\n",
    "    if patience and patience > 0:\n",
    "        callbacks.append(keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True))\n",
    "\n",
    "    m.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    y_pred_s = m.predict(X_val, batch_size=1024, verbose=0).squeeze()\n",
    "    y_pred = unscale_y(y_pred_s)\n",
    "    y_true = y_val_raw.astype(np.float32)\n",
    "\n",
    "    return {\n",
    "        **cfg,\n",
    "        \"val_MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"val_MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"val_DirAcc\": dir_acc(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "# NOTE: this can be slow if you expand the grid.\n",
    "keys = list(grid.keys())\n",
    "configs = [dict(zip(keys, vals)) for vals in product(*[grid[k] for k in keys])]\n",
    "\n",
    "results = []\n",
    "for i, cfg in enumerate(configs[:10]):  # <-- limit to first N configs for a quick run\n",
    "    res = train_eval_one(cfg, epochs=80, patience=10)\n",
    "    results.append(res)\n",
    "    print(i, res[\"val_MAE\"], cfg)\n",
    "\n",
    "df_grid = pd.DataFrame(results).sort_values(\"val_MAE\")\n",
    "df_grid.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b2637",
   "metadata": {},
   "source": [
    "## Notes vs the paper\n",
    "\n",
    "- Table 2 lists the hyperparameter search ranges for LSTM: hidden nodes, hidden layers, learning rate, patience, batch size, dropout, recurrent dropout, loss, optimizer.  \n",
    "- The paper's reported “best” LSTM is: **4 layers**, **5 nodes**, **lr=0.01**, **batch=100**, **dropout=0.4**, **recurrent dropout=0.3**, optimizer **RMSprop**, loss **MAE**, and they use **relu** as activation.  \n",
    "\n",
    "If you want to match their “patient=0” behavior exactly, keep early stopping disabled (as above).  \n",
    "If you want a more stable training loop, use a positive early-stopping patience and monitor `val_loss`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
