{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "48d5911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "from asset_data_module import read_close_prices_all_merged\n",
    "from features import make_feature_windows\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "e1d2ad77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((984, 3), 432)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# markets = ['dow30']\n",
    "markets = ['commodities']\n",
    "start_date, end_date = \"2022-01-01\", \"2025-11-28\"\n",
    "# start_date, end_date = \"2024-06-01\", \"2025-11-28\"\n",
    "\n",
    "_, close_df = read_close_prices_all_merged(markets, after_date=start_date)\n",
    "close_df = close_df.loc[:end_date]\n",
    "\n",
    "rolling = make_feature_windows(\n",
    "    close_prices=close_df,\n",
    "    lookback=60,\n",
    "    horizon=1,\n",
    "    days_per_week=2\n",
    ")\n",
    "close_df.shape, len(rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "22a499ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commodities:GC=F</th>\n",
       "      <th>commodities:HG=F</th>\n",
       "      <th>commodities:SI=F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013907</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>0.015673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015242</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>-0.033202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011948</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>0.018231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.025352</td>\n",
       "      <td>0.015187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004899</td>\n",
       "      <td>-0.035469</td>\n",
       "      <td>0.014236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   commodities:GC=F  commodities:HG=F  commodities:SI=F\n",
       "0          0.013907         -0.002268          0.015673\n",
       "1         -0.015242         -0.000341         -0.033202\n",
       "2          0.011948          0.003967          0.018231\n",
       "3          0.001429          0.025352          0.015187\n",
       "4         -0.004899         -0.035469          0.014236"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rolling[0]['past_weekly_returns'].shape)\n",
    "rolling[0]['past_weekly_returns'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "09518f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom_1w</th>\n",
       "      <th>mom_4w</th>\n",
       "      <th>mom_12w</th>\n",
       "      <th>vol_1w</th>\n",
       "      <th>vol_4w</th>\n",
       "      <th>sharpe_1w</th>\n",
       "      <th>sharpe_4w</th>\n",
       "      <th>vol_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>commodities:GC=F</th>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.614606</td>\n",
       "      <td>0.783788</td>\n",
       "      <td>-0.898660</td>\n",
       "      <td>-1.145738</td>\n",
       "      <td>-0.080740</td>\n",
       "      <td>0.610305</td>\n",
       "      <td>-0.949302</td>\n",
       "      <td>1.151964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commodities:HG=F</th>\n",
       "      <td>-1.132157</td>\n",
       "      <td>-1.153881</td>\n",
       "      <td>-1.126235</td>\n",
       "      <td>1.077268</td>\n",
       "      <td>0.697207</td>\n",
       "      <td>-0.957182</td>\n",
       "      <td>-1.154061</td>\n",
       "      <td>1.043966</td>\n",
       "      <td>-0.644790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commodities:SI=F</th>\n",
       "      <td>0.369445</td>\n",
       "      <td>0.539275</td>\n",
       "      <td>0.342448</td>\n",
       "      <td>-0.178609</td>\n",
       "      <td>0.448531</td>\n",
       "      <td>1.037922</td>\n",
       "      <td>0.543756</td>\n",
       "      <td>-0.094664</td>\n",
       "      <td>-0.507174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mom_1w    mom_4w   mom_12w    vol_1w    vol_4w  sharpe_1w  \\\n",
       "commodities:GC=F  0.762712  0.614606  0.783788 -0.898660 -1.145738  -0.080740   \n",
       "commodities:HG=F -1.132157 -1.153881 -1.126235  1.077268  0.697207  -0.957182   \n",
       "commodities:SI=F  0.369445  0.539275  0.342448 -0.178609  0.448531   1.037922   \n",
       "\n",
       "                  sharpe_4w  vol_ratio  max_drawdown  \n",
       "commodities:GC=F   0.610305  -0.949302      1.151964  \n",
       "commodities:HG=F  -1.154061   1.043966     -0.644790  \n",
       "commodities:SI=F   0.543756  -0.094664     -0.507174  "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['X_feat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "961e94f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "commodities:GC=F   -0.001869\n",
       "commodities:HG=F    0.001322\n",
       "commodities:SI=F   -0.017257\n",
       "dtype: float64"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['y_ret'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "2d2947a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_from_windows(windows, x_key=\"past_weekly_returns\", y_key=\"y_ret\"):\n",
    "    X_list, y_list = [], []\n",
    "    meta_rows = []\n",
    "\n",
    "    for w_idx, w in enumerate(windows):\n",
    "        if x_key == 'past_weekly_returns':\n",
    "            X_df = w[x_key].T          # assets x n_lookback (weeks) (DataFrame)\n",
    "        elif x_key == 'X_feat':\n",
    "            X_df = w[x_key]          # assets x n_features (DataFrame)\n",
    "        y_ser = w[y_key]           # assets (Series or array)\n",
    "\n",
    "        if not isinstance(y_ser, pd.Series):\n",
    "            y_ser = pd.Series(y_ser, index=X_df.index)\n",
    "\n",
    "        assets = X_df.index.intersection(y_ser.index)\n",
    "        Xw = X_df.loc[assets].to_numpy(dtype=np.float32)\n",
    "        yw = y_ser.loc[assets].to_numpy(dtype=np.float32)\n",
    "\n",
    "        mask = np.isfinite(Xw).all(axis=1) & np.isfinite(yw)\n",
    "        Xw, yw = Xw[mask], yw[mask]\n",
    "        assets_kept = assets.to_numpy()[mask]\n",
    "\n",
    "        X_list.append(Xw)\n",
    "        y_list.append(yw)\n",
    "\n",
    "        t0, t1 = w.get(\"t0\", None), w.get(\"t1\", None)\n",
    "        for a in assets_kept:\n",
    "            meta_rows.append((w_idx, a, t0, t1))\n",
    "\n",
    "    X = np.vstack(X_list) ## weeks*assets x n_lookback/n_features\n",
    "    y = np.concatenate(y_list)\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"window_idx\", \"asset\", \"t0\", \"t1\"])\n",
    "    return X, y, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1296, 9),\n",
       " (1296,),\n",
       "    window_idx             asset         t0         t1\n",
       " 0           0  commodities:GC=F 2022-06-24 2022-06-28\n",
       " 1           0  commodities:HG=F 2022-06-24 2022-06-28\n",
       " 2           0  commodities:SI=F 2022-06-24 2022-06-28\n",
       " 3           1  commodities:GC=F 2022-06-28 2022-06-30\n",
       " 4           1  commodities:HG=F 2022-06-28 2022-06-30)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_past_returns, y, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "X, y, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "# X, y, meta = panel_from_windows(rolling, x_key='X_feat')\n",
    "X.shape, y.shape, meta.head() ## len(rolling)*n_asset -- each row is a feature set -- to predict y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4dcd561f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014712454751133919"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = rolling  # your rolling list\n",
    "\n",
    "W = meta[\"window_idx\"].nunique()\n",
    "split_w = int(0.8 * W)\n",
    "\n",
    "train_mask = (meta[\"window_idx\"] < split_w).values\n",
    "test_mask  = (meta[\"window_idx\"] >= split_w).values\n",
    "\n",
    "X_train_raw, y_train = X[train_mask], y[train_mask]\n",
    "X_test_raw,  y_test  = X[test_mask],  y[test_mask]\n",
    "X_past_returns_test_raw = X_past_returns[test_mask]\n",
    "\n",
    "# small validation from the tail of the training windows\n",
    "val_w = max(int(0.1 * split_w), 1)\n",
    "val_start = split_w - val_w\n",
    "val_mask = ((meta[\"window_idx\"] >= val_start) & (meta[\"window_idx\"] < split_w)).values\n",
    "tr2_mask = (meta[\"window_idx\"] < val_start).values\n",
    "\n",
    "X_tr_raw, y_tr = X[tr2_mask], y[tr2_mask]\n",
    "X_va_raw, y_va = X[val_mask], y[val_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr_raw).astype(np.float32)\n",
    "X_va = scaler.transform(X_va_raw).astype(np.float32)\n",
    "X_te = scaler.transform(X_test_raw).astype(np.float32)\n",
    "\n",
    "y_tr = y_tr.astype(np.float32)\n",
    "y_va = y_va.astype(np.float32)\n",
    "y_te = y_test.astype(np.float32)\n",
    "\n",
    "## SCALE Y\n",
    "y_mean = y_tr.mean()\n",
    "y_std  = y_tr.std() + 1e-8\n",
    "\n",
    "y_tr_s = ((y_tr - y_mean) / y_std).astype(np.float32)\n",
    "y_va_s = ((y_va - y_mean) / y_std).astype(np.float32)\n",
    "float(y_std)\n",
    "\n",
    "# X_tr = np.tanh(X_tr)\n",
    "# X_va = np.tanh(X_va)\n",
    "# X_te = np.tanh(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "c0b49e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((933, 9), (102, 9), (261, 9), (933,), (102,), (261,), '87.0 test periods')"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_va.shape, X_te.shape, y_tr.shape, y_va.shape, y_te.shape, f\"{y_te.shape[0]/close_df.shape[1]} test periods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "0569f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.3046 - mae: 0.8552 - val_loss: 1.1037 - val_mae: 0.8108\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2771 - mae: 0.8362 - val_loss: 1.0728 - val_mae: 0.8005\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.2756 - mae: 0.8375 - val_loss: 1.0436 - val_mae: 0.7906\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2325 - mae: 0.8227 - val_loss: 1.0169 - val_mae: 0.7813\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2081 - mae: 0.8143 - val_loss: 0.9922 - val_mae: 0.7728\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1897 - mae: 0.8075 - val_loss: 0.9698 - val_mae: 0.7648\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1768 - mae: 0.8007 - val_loss: 0.9486 - val_mae: 0.7569\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1549 - mae: 0.7996 - val_loss: 0.9292 - val_mae: 0.7495\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1648 - mae: 0.7977 - val_loss: 0.9120 - val_mae: 0.7428\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1664 - mae: 0.7960 - val_loss: 0.8960 - val_mae: 0.7364\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1392 - mae: 0.7900 - val_loss: 0.8812 - val_mae: 0.7302\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1255 - mae: 0.7800 - val_loss: 0.8676 - val_mae: 0.7244\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1268 - mae: 0.7807 - val_loss: 0.8552 - val_mae: 0.7192\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1181 - mae: 0.7741 - val_loss: 0.8441 - val_mae: 0.7144\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1056 - mae: 0.7713 - val_loss: 0.8338 - val_mae: 0.7099\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1295 - mae: 0.7827 - val_loss: 0.8243 - val_mae: 0.7058\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1105 - mae: 0.7732 - val_loss: 0.8151 - val_mae: 0.7019\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0989 - mae: 0.7729 - val_loss: 0.8070 - val_mae: 0.6983\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0690 - mae: 0.7559 - val_loss: 0.7995 - val_mae: 0.6949\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1042 - mae: 0.7747 - val_loss: 0.7920 - val_mae: 0.6914\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0675 - mae: 0.7609 - val_loss: 0.7848 - val_mae: 0.6880\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0705 - mae: 0.7593 - val_loss: 0.7779 - val_mae: 0.6847\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0773 - mae: 0.7675 - val_loss: 0.7717 - val_mae: 0.6816\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0638 - mae: 0.7617 - val_loss: 0.7659 - val_mae: 0.6790\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0673 - mae: 0.7546 - val_loss: 0.7605 - val_mae: 0.6765\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0701 - mae: 0.7573 - val_loss: 0.7556 - val_mae: 0.6744\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0628 - mae: 0.7562 - val_loss: 0.7510 - val_mae: 0.6725\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0711 - mae: 0.7604 - val_loss: 0.7468 - val_mae: 0.6707\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0518 - mae: 0.7522 - val_loss: 0.7427 - val_mae: 0.6689\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0501 - mae: 0.7581 - val_loss: 0.7387 - val_mae: 0.6671\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0473 - mae: 0.7485 - val_loss: 0.7349 - val_mae: 0.6654\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0632 - mae: 0.7564 - val_loss: 0.7315 - val_mae: 0.6641\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0422 - mae: 0.7485 - val_loss: 0.7284 - val_mae: 0.6629\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0489 - mae: 0.7491 - val_loss: 0.7254 - val_mae: 0.6617\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0402 - mae: 0.7456 - val_loss: 0.7227 - val_mae: 0.6606\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0365 - mae: 0.7497 - val_loss: 0.7201 - val_mae: 0.6595\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0580 - mae: 0.7528 - val_loss: 0.7178 - val_mae: 0.6587\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0420 - mae: 0.7484 - val_loss: 0.7158 - val_mae: 0.6579\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0335 - mae: 0.7458 - val_loss: 0.7138 - val_mae: 0.6570\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0289 - mae: 0.7495 - val_loss: 0.7118 - val_mae: 0.6561\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0337 - mae: 0.7458 - val_loss: 0.7098 - val_mae: 0.6552\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0356 - mae: 0.7461 - val_loss: 0.7080 - val_mae: 0.6543\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0320 - mae: 0.7392 - val_loss: 0.7062 - val_mae: 0.6536\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0268 - mae: 0.7401 - val_loss: 0.7046 - val_mae: 0.6528\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0213 - mae: 0.7376 - val_loss: 0.7031 - val_mae: 0.6521\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0262 - mae: 0.7470 - val_loss: 0.7020 - val_mae: 0.6515\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0195 - mae: 0.7418 - val_loss: 0.7009 - val_mae: 0.6509\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0302 - mae: 0.7449 - val_loss: 0.7001 - val_mae: 0.6506\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0195 - mae: 0.7421 - val_loss: 0.6990 - val_mae: 0.6501\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0302 - mae: 0.7429 - val_loss: 0.6979 - val_mae: 0.6495\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0320 - mae: 0.7440 - val_loss: 0.6967 - val_mae: 0.6490\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0212 - mae: 0.7403 - val_loss: 0.6955 - val_mae: 0.6484\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0387 - mae: 0.7421 - val_loss: 0.6945 - val_mae: 0.6480\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0239 - mae: 0.7402 - val_loss: 0.6935 - val_mae: 0.6475\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0214 - mae: 0.7379 - val_loss: 0.6924 - val_mae: 0.6468\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0408 - mae: 0.7481 - val_loss: 0.6913 - val_mae: 0.6462\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0295 - mae: 0.7419 - val_loss: 0.6901 - val_mae: 0.6456\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0313 - mae: 0.7352 - val_loss: 0.6890 - val_mae: 0.6451\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0314 - mae: 0.7373 - val_loss: 0.6883 - val_mae: 0.6448\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0304 - mae: 0.7455 - val_loss: 0.6874 - val_mae: 0.6444\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0127 - mae: 0.7374 - val_loss: 0.6870 - val_mae: 0.6442\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0087 - mae: 0.7347 - val_loss: 0.6865 - val_mae: 0.6440\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0369 - mae: 0.7472 - val_loss: 0.6860 - val_mae: 0.6438\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0321 - mae: 0.7469 - val_loss: 0.6853 - val_mae: 0.6435\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0029 - mae: 0.7331 - val_loss: 0.6847 - val_mae: 0.6432\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0188 - mae: 0.7421 - val_loss: 0.6842 - val_mae: 0.6429\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0199 - mae: 0.7404 - val_loss: 0.6838 - val_mae: 0.6427\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0357 - mae: 0.7405 - val_loss: 0.6835 - val_mae: 0.6425\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0183 - mae: 0.7369 - val_loss: 0.6833 - val_mae: 0.6424\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0057 - mae: 0.7358 - val_loss: 0.6829 - val_mae: 0.6422\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0301 - mae: 0.7404 - val_loss: 0.6826 - val_mae: 0.6420\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0178 - mae: 0.7351 - val_loss: 0.6823 - val_mae: 0.6419\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0139 - mae: 0.7395 - val_loss: 0.6819 - val_mae: 0.6416\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0058 - mae: 0.7363 - val_loss: 0.6815 - val_mae: 0.6414\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0221 - mae: 0.7424 - val_loss: 0.6811 - val_mae: 0.6412\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0046 - mae: 0.7294 - val_loss: 0.6803 - val_mae: 0.6407\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0093 - mae: 0.7359 - val_loss: 0.6798 - val_mae: 0.6404\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9985 - mae: 0.7269 - val_loss: 0.6796 - val_mae: 0.6403\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9995 - mae: 0.7369 - val_loss: 0.6794 - val_mae: 0.6402\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9927 - mae: 0.7331 - val_loss: 0.6791 - val_mae: 0.6401\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0129 - mae: 0.7354 - val_loss: 0.6787 - val_mae: 0.6398\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0036 - mae: 0.7326 - val_loss: 0.6785 - val_mae: 0.6397\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0112 - mae: 0.7391 - val_loss: 0.6782 - val_mae: 0.6395\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9822 - mae: 0.7241 - val_loss: 0.6780 - val_mae: 0.6394\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0086 - mae: 0.7306 - val_loss: 0.6777 - val_mae: 0.6392\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9997 - mae: 0.7295 - val_loss: 0.6772 - val_mae: 0.6390\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0084 - mae: 0.7329 - val_loss: 0.6767 - val_mae: 0.6388\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0054 - mae: 0.7365 - val_loss: 0.6762 - val_mae: 0.6385\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9994 - mae: 0.7300 - val_loss: 0.6761 - val_mae: 0.6383\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9908 - mae: 0.7282 - val_loss: 0.6760 - val_mae: 0.6382\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0016 - mae: 0.7364 - val_loss: 0.6757 - val_mae: 0.6381\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0083 - mae: 0.7385 - val_loss: 0.6753 - val_mae: 0.6379\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9902 - mae: 0.7287 - val_loss: 0.6752 - val_mae: 0.6378\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0066 - mae: 0.7344 - val_loss: 0.6750 - val_mae: 0.6377\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0007 - mae: 0.7316 - val_loss: 0.6747 - val_mae: 0.6376\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0150 - mae: 0.7377 - val_loss: 0.6745 - val_mae: 0.6376\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9910 - mae: 0.7294 - val_loss: 0.6745 - val_mae: 0.6377\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0049 - mae: 0.7357 - val_loss: 0.6746 - val_mae: 0.6378\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9880 - mae: 0.7293 - val_loss: 0.6745 - val_mae: 0.6377\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0026 - mae: 0.7300 - val_loss: 0.6742 - val_mae: 0.6376\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(in_dim, hidden=(64, 32), dropout=0.1, lr=1e-3):\n",
    "    inputs = keras.Input(shape=(in_dim,))\n",
    "    x = inputs\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation=\"relu\",)(x) #kernel_regularizer=regularizers.l2(1e-4)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_mlp(in_dim=X_tr.shape[1], hidden=(32, 16), dropout=0.1, lr=1e-4)\n",
    "# model = build_mlp(in_dim=X_tr.shape[1], hidden=(15, 15, 15, 15, 15, 15), dropout=0.0, lr=1e-4)\n",
    "# model = build_mlp(in_dim=X_tr.shape[1], hidden=(15, 15, 15, 15, 15, 15), dropout=0.0, lr=0.01)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    # X_tr, y_tr,\n",
    "    X_tr, y_tr_s,   ## Scaled y\n",
    "    # validation_data=(X_va, y_va),\n",
    "    validation_data=(X_va, y_va_s),   ## Scaled y\n",
    "    # epochs=150,\n",
    "    epochs=100,\n",
    "    # batch_size=256,\n",
    "    batch_size=100,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "55b4068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def prediction_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    sign_acc = (np.sign(y_true) == np.sign(y_pred)).mean()\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan\n",
    "    \n",
    "    return {\"MSE\": round(mse, 5), \"MAE\": round(mae, 5), \"R2\": round(r2, 5), \"SignAcc\": round(float(sign_acc), 5), \"Corr\": round(float(corr), 5)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "841800ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean\n",
      "{'MSE': 0.00052, 'MAE': 0.01233, 'R2': -0.01059, 'SignAcc': 0.54406, 'Corr': 0.00561}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "MLP\n",
      "{'MSE': 0.00051, 'MAE': 0.01241, 'R2': -0.00266, 'SignAcc': 0.51724, 'Corr': 0.03108}\n",
      "0.0011402754\n"
     ]
    }
   ],
   "source": [
    "y_pred_sm = X_past_returns_test_raw.astype(np.float32).mean(axis=1)\n",
    "# X_te_raw = scaler.inverse_transform(X_te)\n",
    "# y_pred_sm = X_te_raw.mean(axis=1)\n",
    "print(\"Sample Mean\")\n",
    "print(prediction_metrics(y_te, y_pred_sm))\n",
    "\n",
    "# y_pred = model.predict(X_te, batch_size=1024).squeeze()\n",
    "## Scaled y:\n",
    "pred_s = model.predict(X_te, batch_size=1024).squeeze()\n",
    "y_pred = y_mean + y_std * pred_s\n",
    "\n",
    "print(\"MLP\")\n",
    "print(prediction_metrics(y_te, y_pred))\n",
    "\n",
    "print(y_te.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "75af9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_sample_mean(test_window):\n",
    "    # past_weekly_returns: (lookback periods) x (assets)\n",
    "    return test_window[\"past_weekly_returns\"].mean(axis=0)  # pd.Series indexed by asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35858728",
   "metadata": {},
   "source": [
    "Walk-Forward Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "f2307a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_eval_mlp(\n",
    "    rolling,\n",
    "    train_len=150,          # number of windows to train on each step\n",
    "    x_key=\"X_feat\",\n",
    "    y_key=\"y_ret\",\n",
    "    hidden=(64, 32),\n",
    "    dropout=0.1,\n",
    "    lr=3e-4,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    seed=42\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    all_rows = []\n",
    "    week_metrics = []\n",
    "\n",
    "    for i in range(train_len, len(rolling)):\n",
    "        train_windows = rolling[i-train_len:i]\n",
    "        test_window   = rolling[i]\n",
    "\n",
    "        # --- build train panel ---\n",
    "        X_train, y_train, meta = panel_from_windows(train_windows, x_key=x_key, y_key=y_key)\n",
    "        if X_train.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # --- scaler on TRAIN only ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "        # --- build test cross-section ---\n",
    "        X_test_df = test_window[x_key]            # assets x features\n",
    "        y_test = test_window[y_key]\n",
    "        if not isinstance(y_test, pd.Series):\n",
    "            y_test = pd.Series(y_test, index=X_test_df.index)\n",
    "\n",
    "        assets = X_test_df.index.intersection(y_test.index).sort_values()\n",
    "        X_test = X_test_df.loc[assets].to_numpy(np.float32)\n",
    "        y_true = y_test.loc[assets].to_numpy(np.float32)\n",
    "\n",
    "        mask = np.isfinite(X_test).all(axis=1) & np.isfinite(y_true)\n",
    "        assets = assets[mask]\n",
    "        X_test = X_test[mask]\n",
    "        y_true = y_true[mask]\n",
    "\n",
    "        X_test_sc = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "        # --- train model (fresh each step) ---\n",
    "        model = build_mlp(in_dim=X_train_sc.shape[1], hidden=hidden, dropout=dropout, lr=lr)\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_sc, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "\n",
    "        # --- predict ---\n",
    "        y_pred = model.predict(X_test_sc, batch_size=1024, verbose=0).squeeze()\n",
    "\n",
    "        # --- baseline: sample mean of past period returns ---\n",
    "        y_pred_sm_ser = baseline_sample_mean(test_window).loc[assets]\n",
    "        y_pred_sm = y_pred_sm_ser.to_numpy(np.float32)\n",
    "\n",
    "        # store per-asset predictions\n",
    "        for a, yt, yp, ypsm in zip(assets, y_true, y_pred, y_pred_sm):\n",
    "            all_rows.append(\n",
    "                {\"window_idx\": i, \"asset\": a, \"y\": float(yt), \"pred_mlp\": float(yp), \"pred_sm\": float(ypsm)}\n",
    "            )\n",
    "\n",
    "        # per-window metrics (cross-section)\n",
    "        m_mlp = prediction_metrics(y_true, y_pred)\n",
    "        m_sm  = prediction_metrics(y_true, y_pred_sm)\n",
    "        week_metrics.append({\"window_idx\": i, **{f\"mlp_{k}\": v for k,v in m_mlp.items()},\n",
    "                                **{f\"sm_{k}\": v for k,v in m_sm.items()}})\n",
    "\n",
    "    preds_df = pd.DataFrame(all_rows)\n",
    "    week_df  = pd.DataFrame(week_metrics)\n",
    "\n",
    "    # pooled metrics over all (window, asset) test points\n",
    "    pooled_mlp = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_mlp\"])\n",
    "    pooled_sm  = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_sm\"])\n",
    "\n",
    "    return preds_df, week_df, pooled_mlp, pooled_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "394ebbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_df, week_df, pooled_mlp, pooled_sm = walk_forward_eval_mlp(\n",
    "#     rolling=rolling,\n",
    "#     train_len=100,   # e.g., last 52 periods (with days_per_week=2 that's ~104 trading days)\n",
    "#     x_key=\"X_feat\",\n",
    "#     y_key=\"y_ret\",\n",
    "#     hidden=(16,8)\n",
    "# )\n",
    "\n",
    "# print(\"Pooled MLP:\", pooled_mlp)\n",
    "# print(\"Pooled  SM:\", pooled_sm)\n",
    "# print(\"Weekly win-rate (MAE):\", (week_df[\"mlp_MAE\"] < week_df[\"sm_MAE\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a76b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
