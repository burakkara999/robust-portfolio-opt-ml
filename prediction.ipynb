{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "48d5911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "from asset_data_module import read_close_prices_all_merged\n",
    "from features import make_feature_windows\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2ad77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((981, 30), 430)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markets = ['dow30', 'commodities']\n",
    "# markets = ['dow30']\n",
    "# markets = ['commodities']\n",
    "start_date, end_date = \"2022-01-01\", \"2025-11-28\"\n",
    "# start_date, end_date = \"2024-06-01\", \"2025-11-28\"\n",
    "\n",
    "_, close_df = read_close_prices_all_merged(markets, after_date=start_date)\n",
    "close_df = close_df.loc[:end_date]\n",
    "\n",
    "rolling = make_feature_windows(\n",
    "    close_prices=close_df,\n",
    "    lookback=60,\n",
    "    horizon=1,\n",
    "    days_per_week=2\n",
    ")\n",
    "close_df.shape, len(rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "22a499ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow30:AAPL</th>\n",
       "      <th>dow30:AMGN</th>\n",
       "      <th>dow30:AXP</th>\n",
       "      <th>dow30:BA</th>\n",
       "      <th>dow30:CAT</th>\n",
       "      <th>dow30:CRM</th>\n",
       "      <th>dow30:CSCO</th>\n",
       "      <th>dow30:CVX</th>\n",
       "      <th>dow30:DIS</th>\n",
       "      <th>dow30:DOW</th>\n",
       "      <th>...</th>\n",
       "      <th>dow30:MSFT</th>\n",
       "      <th>dow30:NKE</th>\n",
       "      <th>dow30:PFE</th>\n",
       "      <th>dow30:PG</th>\n",
       "      <th>dow30:RTX</th>\n",
       "      <th>dow30:TRV</th>\n",
       "      <th>dow30:UNH</th>\n",
       "      <th>dow30:V</th>\n",
       "      <th>dow30:VZ</th>\n",
       "      <th>dow30:WMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039733</td>\n",
       "      <td>-0.006861</td>\n",
       "      <td>0.020885</td>\n",
       "      <td>0.024756</td>\n",
       "      <td>0.059772</td>\n",
       "      <td>-0.115169</td>\n",
       "      <td>-0.040796</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>-0.010066</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056440</td>\n",
       "      <td>-0.014805</td>\n",
       "      <td>-0.018169</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>-0.025385</td>\n",
       "      <td>-0.006479</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>-0.005060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015846</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.017646</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.014002</td>\n",
       "      <td>0.022731</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007424</td>\n",
       "      <td>-0.033084</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>-0.008992</td>\n",
       "      <td>0.009666</td>\n",
       "      <td>0.039697</td>\n",
       "      <td>-0.065594</td>\n",
       "      <td>-0.013915</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>0.006717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016761</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>-0.019094</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>-0.043421</td>\n",
       "      <td>0.017259</td>\n",
       "      <td>-0.025390</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>-0.018701</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>-0.011963</td>\n",
       "      <td>-0.012056</td>\n",
       "      <td>-0.004773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016645</td>\n",
       "      <td>-0.006606</td>\n",
       "      <td>-0.011988</td>\n",
       "      <td>0.035828</td>\n",
       "      <td>0.031550</td>\n",
       "      <td>-0.026799</td>\n",
       "      <td>-0.014047</td>\n",
       "      <td>-0.009185</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032853</td>\n",
       "      <td>-0.004735</td>\n",
       "      <td>-0.020494</td>\n",
       "      <td>-0.002335</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>-0.000674</td>\n",
       "      <td>-0.003353</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>0.008769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.013977</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>-0.041955</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>-0.011305</td>\n",
       "      <td>-0.029203</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>-0.020605</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007079</td>\n",
       "      <td>-0.017806</td>\n",
       "      <td>-0.026084</td>\n",
       "      <td>-0.009904</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>-0.013873</td>\n",
       "      <td>0.003297</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>-0.020487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dow30:AAPL  dow30:AMGN  dow30:AXP  dow30:BA  dow30:CAT  dow30:CRM  \\\n",
       "0   -0.039733   -0.006861   0.020885  0.024756   0.059772  -0.115169   \n",
       "1   -0.015846    0.009504   0.017646  0.011340   0.020003   0.002807   \n",
       "2    0.016761    0.022147   0.005718  0.002410  -0.019094   0.028200   \n",
       "3   -0.016645   -0.006606  -0.011988  0.035828   0.031550  -0.026799   \n",
       "4   -0.013977    0.012826  -0.041955  0.004945   0.012564  -0.011305   \n",
       "\n",
       "   dow30:CSCO  dow30:CVX  dow30:DIS  dow30:DOW  ...  dow30:MSFT  dow30:NKE  \\\n",
       "0   -0.040796   0.024517  -0.010066   0.025691  ...   -0.056440  -0.014805   \n",
       "1    0.014002   0.022731   0.016868   0.012772  ...   -0.007424  -0.033084   \n",
       "2    0.020082   0.023242   0.000380   0.001353  ...    0.002989  -0.043421   \n",
       "3   -0.014047  -0.009185  -0.015639   0.012760  ...   -0.032853  -0.004735   \n",
       "4   -0.029203   0.020143  -0.020605   0.004328  ...   -0.007079  -0.017806   \n",
       "\n",
       "   dow30:PFE  dow30:PG  dow30:RTX  dow30:TRV  dow30:UNH   dow30:V  dow30:VZ  \\\n",
       "0  -0.018169  0.008010   0.029457   0.025483  -0.025385 -0.006479  0.029685   \n",
       "1   0.001617 -0.008992   0.009666   0.039697  -0.065594 -0.013915  0.016041   \n",
       "2   0.017259 -0.025390   0.003091  -0.018701   0.022424 -0.011963 -0.012056   \n",
       "3  -0.020494 -0.002335   0.009217  -0.000674  -0.003353  0.002888 -0.001307   \n",
       "4  -0.026084 -0.009904  -0.008886   0.003184  -0.013873  0.003297 -0.001683   \n",
       "\n",
       "   dow30:WMT  \n",
       "0  -0.005060  \n",
       "1   0.006717  \n",
       "2  -0.004773  \n",
       "3   0.008769  \n",
       "4  -0.020487  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rolling[0]['past_weekly_returns'].shape)\n",
    "rolling[0]['past_weekly_returns'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "09518f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom_1w</th>\n",
       "      <th>mom_4w</th>\n",
       "      <th>mom_12w</th>\n",
       "      <th>vol_1w</th>\n",
       "      <th>vol_4w</th>\n",
       "      <th>sharpe_1w</th>\n",
       "      <th>sharpe_4w</th>\n",
       "      <th>vol_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dow30:AAPL</th>\n",
       "      <td>0.549830</td>\n",
       "      <td>0.655922</td>\n",
       "      <td>0.334903</td>\n",
       "      <td>-1.335888</td>\n",
       "      <td>0.073495</td>\n",
       "      <td>3.354612</td>\n",
       "      <td>0.664756</td>\n",
       "      <td>-1.441571</td>\n",
       "      <td>-0.377131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:AMGN</th>\n",
       "      <td>-0.395536</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.007335</td>\n",
       "      <td>-1.336021</td>\n",
       "      <td>-1.307819</td>\n",
       "      <td>1.230191</td>\n",
       "      <td>0.538996</td>\n",
       "      <td>-1.304322</td>\n",
       "      <td>1.442540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:AXP</th>\n",
       "      <td>-0.766660</td>\n",
       "      <td>-0.432284</td>\n",
       "      <td>-0.666186</td>\n",
       "      <td>0.774774</td>\n",
       "      <td>1.454327</td>\n",
       "      <td>-0.617898</td>\n",
       "      <td>-0.598703</td>\n",
       "      <td>0.164282</td>\n",
       "      <td>-0.591565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:BA</th>\n",
       "      <td>-0.009124</td>\n",
       "      <td>2.586184</td>\n",
       "      <td>1.253099</td>\n",
       "      <td>2.017293</td>\n",
       "      <td>2.145951</td>\n",
       "      <td>-0.577235</td>\n",
       "      <td>1.511861</td>\n",
       "      <td>0.233711</td>\n",
       "      <td>-2.413837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:CAT</th>\n",
       "      <td>-1.736866</td>\n",
       "      <td>-2.056154</td>\n",
       "      <td>-1.342731</td>\n",
       "      <td>2.418059</td>\n",
       "      <td>1.560400</td>\n",
       "      <td>-0.715582</td>\n",
       "      <td>-1.877875</td>\n",
       "      <td>1.526759</td>\n",
       "      <td>0.094049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mom_1w    mom_4w   mom_12w    vol_1w    vol_4w  sharpe_1w  \\\n",
       "dow30:AAPL  0.549830  0.655922  0.334903 -1.335888  0.073495   3.354612   \n",
       "dow30:AMGN -0.395536  0.117647 -0.007335 -1.336021 -1.307819   1.230191   \n",
       "dow30:AXP  -0.766660 -0.432284 -0.666186  0.774774  1.454327  -0.617898   \n",
       "dow30:BA   -0.009124  2.586184  1.253099  2.017293  2.145951  -0.577235   \n",
       "dow30:CAT  -1.736866 -2.056154 -1.342731  2.418059  1.560400  -0.715582   \n",
       "\n",
       "            sharpe_4w  vol_ratio  max_drawdown  \n",
       "dow30:AAPL   0.664756  -1.441571     -0.377131  \n",
       "dow30:AMGN   0.538996  -1.304322      1.442540  \n",
       "dow30:AXP   -0.598703   0.164282     -0.591565  \n",
       "dow30:BA     1.511861   0.233711     -2.413837  \n",
       "dow30:CAT   -1.877875   1.526759      0.094049  "
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['X_feat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "961e94f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dow30:AAPL   -0.030242\n",
       "dow30:AMGN   -0.005488\n",
       "dow30:AXP    -0.014661\n",
       "dtype: float64"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['y_ret'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "2d2947a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_from_windows(windows, x_key=\"past_weekly_returns\", y_key=\"y_ret\"):\n",
    "    X_list, y_list = [], []\n",
    "    meta_rows = []\n",
    "\n",
    "    for w_idx, w in enumerate(windows):\n",
    "        if x_key == 'past_weekly_returns':\n",
    "            X_df = w[x_key].T          # assets x n_lookback (weeks) (DataFrame)\n",
    "        elif x_key == 'X_feat':\n",
    "            X_df = w[x_key]          # assets x n_features (DataFrame)\n",
    "        y_ser = w[y_key]           # assets (Series or array)\n",
    "\n",
    "        if not isinstance(y_ser, pd.Series):\n",
    "            y_ser = pd.Series(y_ser, index=X_df.index)\n",
    "\n",
    "        assets = X_df.index.intersection(y_ser.index)\n",
    "        Xw = X_df.loc[assets].to_numpy(dtype=np.float32)\n",
    "        yw = y_ser.loc[assets].to_numpy(dtype=np.float32)\n",
    "\n",
    "        mask = np.isfinite(Xw).all(axis=1) & np.isfinite(yw)\n",
    "        Xw, yw = Xw[mask], yw[mask]\n",
    "        assets_kept = assets.to_numpy()[mask]\n",
    "\n",
    "        X_list.append(Xw)\n",
    "        y_list.append(yw)\n",
    "\n",
    "        t0, t1 = w.get(\"t0\", None), w.get(\"t1\", None)\n",
    "        for a in assets_kept:\n",
    "            meta_rows.append((w_idx, a, t0, t1))\n",
    "\n",
    "    X = np.vstack(X_list) ## weeks*assets x n_lookback/n_features\n",
    "    y = np.concatenate(y_list)\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"window_idx\", \"asset\", \"t0\", \"t1\"])\n",
    "    return X, y, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "00e8a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12900, 60),\n",
       " (12900,),\n",
       "    window_idx       asset         t0         t1\n",
       " 0           0  dow30:AAPL 2022-06-24 2022-06-28\n",
       " 1           0  dow30:AMGN 2022-06-24 2022-06-28\n",
       " 2           0   dow30:AXP 2022-06-24 2022-06-28\n",
       " 3           0    dow30:BA 2022-06-24 2022-06-28\n",
       " 4           0   dow30:CAT 2022-06-24 2022-06-28)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_past_returns, y, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "X, y, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "# X, y, meta = panel_from_windows(rolling, x_key='X_feat')\n",
    "X.shape, y.shape, meta.head() ## len(rolling)*n_asset -- each row is a feature set -- to predict y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "4dcd561f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015860024839639664"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = rolling  # your rolling list\n",
    "\n",
    "W = meta[\"window_idx\"].nunique()\n",
    "split_w = int(0.8 * W)\n",
    "\n",
    "train_mask = (meta[\"window_idx\"] < split_w).values\n",
    "test_mask  = (meta[\"window_idx\"] >= split_w).values\n",
    "\n",
    "X_train_raw, y_train = X[train_mask], y[train_mask]\n",
    "X_test_raw,  y_test  = X[test_mask],  y[test_mask]\n",
    "X_past_returns_test_raw = X_past_returns[test_mask]\n",
    "\n",
    "# small validation from the tail of the training windows\n",
    "val_w = max(int(0.1 * split_w), 1)\n",
    "val_start = split_w - val_w\n",
    "val_mask = ((meta[\"window_idx\"] >= val_start) & (meta[\"window_idx\"] < split_w)).values\n",
    "tr2_mask = (meta[\"window_idx\"] < val_start).values\n",
    "\n",
    "X_tr_raw, y_tr = X[tr2_mask], y[tr2_mask]\n",
    "X_va_raw, y_va = X[val_mask], y[val_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr_raw).astype(np.float32)\n",
    "X_va = scaler.transform(X_va_raw).astype(np.float32)\n",
    "X_te = scaler.transform(X_test_raw).astype(np.float32)\n",
    "\n",
    "y_tr = y_tr.astype(np.float32)\n",
    "y_va = y_va.astype(np.float32)\n",
    "y_te = y_test.astype(np.float32)\n",
    "\n",
    "## SCALE Y\n",
    "y_mean = y_tr.mean()\n",
    "y_std  = y_tr.std() + 1e-8\n",
    "\n",
    "y_tr_s = ((y_tr - y_mean) / y_std).astype(np.float32)\n",
    "y_va_s = ((y_va - y_mean) / y_std).astype(np.float32)\n",
    "float(y_std)\n",
    "\n",
    "# X_tr = np.tanh(X_tr)\n",
    "# X_va = np.tanh(X_va)\n",
    "# X_te = np.tanh(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "c0b49e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9300, 60),\n",
       " (1020, 60),\n",
       " (2580, 60),\n",
       " (9300,),\n",
       " (1020,),\n",
       " (2580,),\n",
       " '86.0 test periods')"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_va.shape, X_te.shape, y_tr.shape, y_va.shape, y_te.shape, f\"{y_te.shape[0]/close_df.shape[1]} test periods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "0569f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.4256 - mae: 0.8625 - val_loss: 1.4019 - val_mae: 0.8437\n",
      "Epoch 2/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 1.2952 - mae: 0.8165 - val_loss: 1.3500 - val_mae: 0.8231\n",
      "Epoch 3/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 1.2679 - mae: 0.8081 - val_loss: 1.3139 - val_mae: 0.8075\n",
      "Epoch 4/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 1.2196 - mae: 0.7897 - val_loss: 1.2868 - val_mae: 0.7958\n",
      "Epoch 5/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 1.1870 - mae: 0.7735 - val_loss: 1.2660 - val_mae: 0.7860\n",
      "Epoch 6/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 1.1455 - mae: 0.7593 - val_loss: 1.2522 - val_mae: 0.7792\n",
      "Epoch 7/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 1.1266 - mae: 0.7503 - val_loss: 1.2405 - val_mae: 0.7732\n",
      "Epoch 8/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 1.1093 - mae: 0.7431 - val_loss: 1.2315 - val_mae: 0.7688\n",
      "Epoch 9/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 1.0928 - mae: 0.7345 - val_loss: 1.2235 - val_mae: 0.7648\n",
      "Epoch 10/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 1.0753 - mae: 0.7314 - val_loss: 1.2165 - val_mae: 0.7615\n",
      "Epoch 11/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 1.0717 - mae: 0.7293 - val_loss: 1.2101 - val_mae: 0.7585\n",
      "Epoch 12/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 1.0649 - mae: 0.7267 - val_loss: 1.2053 - val_mae: 0.7561\n",
      "Epoch 13/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 1.0475 - mae: 0.7177 - val_loss: 1.2012 - val_mae: 0.7542\n",
      "Epoch 14/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 1.0440 - mae: 0.7178 - val_loss: 1.1972 - val_mae: 0.7523\n",
      "Epoch 15/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 1.0343 - mae: 0.7132 - val_loss: 1.1944 - val_mae: 0.7510\n",
      "Epoch 16/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 1.0316 - mae: 0.7133 - val_loss: 1.1913 - val_mae: 0.7498\n",
      "Epoch 17/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 1.0321 - mae: 0.7103 - val_loss: 1.1898 - val_mae: 0.7489\n",
      "Epoch 18/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 1.0221 - mae: 0.7093 - val_loss: 1.1873 - val_mae: 0.7477\n",
      "Epoch 19/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 1.0192 - mae: 0.7054 - val_loss: 1.1853 - val_mae: 0.7468\n",
      "Epoch 20/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.0100 - mae: 0.7022 - val_loss: 1.1838 - val_mae: 0.7462\n",
      "Epoch 21/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 1.0087 - mae: 0.7029 - val_loss: 1.1822 - val_mae: 0.7457\n",
      "Epoch 22/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 1.0103 - mae: 0.7026 - val_loss: 1.1811 - val_mae: 0.7450\n",
      "Epoch 23/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 1.0009 - mae: 0.6996 - val_loss: 1.1797 - val_mae: 0.7444\n",
      "Epoch 24/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 1.0013 - mae: 0.7002 - val_loss: 1.1785 - val_mae: 0.7440\n",
      "Epoch 25/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.9958 - mae: 0.6985 - val_loss: 1.1772 - val_mae: 0.7435\n",
      "Epoch 26/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.9967 - mae: 0.6970 - val_loss: 1.1760 - val_mae: 0.7430\n",
      "Epoch 27/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.9934 - mae: 0.6961 - val_loss: 1.1745 - val_mae: 0.7424\n",
      "Epoch 28/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.9854 - mae: 0.6923 - val_loss: 1.1738 - val_mae: 0.7420\n",
      "Epoch 29/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.9832 - mae: 0.6906 - val_loss: 1.1730 - val_mae: 0.7419\n",
      "Epoch 30/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.9759 - mae: 0.6891 - val_loss: 1.1728 - val_mae: 0.7418\n",
      "Epoch 31/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 0.9744 - mae: 0.6889 - val_loss: 1.1720 - val_mae: 0.7415\n",
      "Epoch 32/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.9748 - mae: 0.6901 - val_loss: 1.1714 - val_mae: 0.7415\n",
      "Epoch 33/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.9772 - mae: 0.6891 - val_loss: 1.1707 - val_mae: 0.7411\n",
      "Epoch 34/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.9683 - mae: 0.6864 - val_loss: 1.1702 - val_mae: 0.7412\n",
      "Epoch 35/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.9748 - mae: 0.6891 - val_loss: 1.1690 - val_mae: 0.7409\n",
      "Epoch 36/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.9774 - mae: 0.6892 - val_loss: 1.1677 - val_mae: 0.7406\n",
      "Epoch 37/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.9632 - mae: 0.6831 - val_loss: 1.1675 - val_mae: 0.7404\n",
      "Epoch 38/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.9655 - mae: 0.6839 - val_loss: 1.1669 - val_mae: 0.7402\n",
      "Epoch 39/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.9577 - mae: 0.6836 - val_loss: 1.1665 - val_mae: 0.7403\n",
      "Epoch 40/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.9589 - mae: 0.6833 - val_loss: 1.1666 - val_mae: 0.7404\n",
      "Epoch 41/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.9535 - mae: 0.6815 - val_loss: 1.1658 - val_mae: 0.7404\n",
      "Epoch 42/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.9571 - mae: 0.6821 - val_loss: 1.1652 - val_mae: 0.7402\n",
      "Epoch 43/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.9494 - mae: 0.6771 - val_loss: 1.1653 - val_mae: 0.7406\n",
      "Epoch 44/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.9495 - mae: 0.6797 - val_loss: 1.1644 - val_mae: 0.7405\n",
      "Epoch 45/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.9507 - mae: 0.6790 - val_loss: 1.1635 - val_mae: 0.7404\n",
      "Epoch 46/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.9475 - mae: 0.6789 - val_loss: 1.1628 - val_mae: 0.7404\n",
      "Epoch 47/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.9478 - mae: 0.6780 - val_loss: 1.1625 - val_mae: 0.7405\n",
      "Epoch 48/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 0.9420 - mae: 0.6763 - val_loss: 1.1616 - val_mae: 0.7405\n",
      "Epoch 49/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.9460 - mae: 0.6759 - val_loss: 1.1610 - val_mae: 0.7407\n",
      "Epoch 50/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.9488 - mae: 0.6781 - val_loss: 1.1598 - val_mae: 0.7404\n",
      "Epoch 51/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.9386 - mae: 0.6750 - val_loss: 1.1592 - val_mae: 0.7403\n",
      "Epoch 52/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.9352 - mae: 0.6750 - val_loss: 1.1590 - val_mae: 0.7404\n",
      "Epoch 53/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.9288 - mae: 0.6726 - val_loss: 1.1587 - val_mae: 0.7404\n",
      "Epoch 54/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.9369 - mae: 0.6748 - val_loss: 1.1583 - val_mae: 0.7404\n",
      "Epoch 55/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.9324 - mae: 0.6719 - val_loss: 1.1581 - val_mae: 0.7405\n",
      "Epoch 56/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.9211 - mae: 0.6701 - val_loss: 1.1579 - val_mae: 0.7408\n",
      "Epoch 57/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.9273 - mae: 0.6714 - val_loss: 1.1572 - val_mae: 0.7407\n",
      "Epoch 58/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 0.9203 - mae: 0.6691 - val_loss: 1.1574 - val_mae: 0.7410\n",
      "Epoch 59/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.9219 - mae: 0.6703 - val_loss: 1.1566 - val_mae: 0.7410\n",
      "Epoch 60/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.9266 - mae: 0.6694 - val_loss: 1.1569 - val_mae: 0.7410\n",
      "Epoch 61/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.9257 - mae: 0.6709 - val_loss: 1.1562 - val_mae: 0.7407\n",
      "Epoch 62/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.9201 - mae: 0.6681 - val_loss: 1.1563 - val_mae: 0.7409\n",
      "Epoch 63/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.9173 - mae: 0.6683 - val_loss: 1.1564 - val_mae: 0.7412\n",
      "Epoch 64/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.9131 - mae: 0.6668 - val_loss: 1.1564 - val_mae: 0.7412\n",
      "Epoch 65/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.9126 - mae: 0.6663 - val_loss: 1.1558 - val_mae: 0.7414\n",
      "Epoch 66/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.9180 - mae: 0.6663 - val_loss: 1.1554 - val_mae: 0.7412\n",
      "Epoch 67/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 0.9084 - mae: 0.6648 - val_loss: 1.1555 - val_mae: 0.7411\n",
      "Epoch 68/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.9124 - mae: 0.6675 - val_loss: 1.1552 - val_mae: 0.7411\n",
      "Epoch 69/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.9160 - mae: 0.6673 - val_loss: 1.1548 - val_mae: 0.7409\n",
      "Epoch 70/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.9116 - mae: 0.6651 - val_loss: 1.1548 - val_mae: 0.7413\n",
      "Epoch 71/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.9074 - mae: 0.6656 - val_loss: 1.1546 - val_mae: 0.7415\n",
      "Epoch 72/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.9066 - mae: 0.6624 - val_loss: 1.1549 - val_mae: 0.7416\n",
      "Epoch 73/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.9102 - mae: 0.6652 - val_loss: 1.1545 - val_mae: 0.7415\n",
      "Epoch 74/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 0.9153 - mae: 0.6670 - val_loss: 1.1543 - val_mae: 0.7414\n",
      "Epoch 75/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.9005 - mae: 0.6624 - val_loss: 1.1542 - val_mae: 0.7415\n",
      "Epoch 76/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 0.9026 - mae: 0.6634 - val_loss: 1.1531 - val_mae: 0.7410\n",
      "Epoch 77/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.8959 - mae: 0.6610 - val_loss: 1.1529 - val_mae: 0.7412\n",
      "Epoch 78/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.8935 - mae: 0.6606 - val_loss: 1.1528 - val_mae: 0.7415\n",
      "Epoch 79/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.8962 - mae: 0.6594 - val_loss: 1.1529 - val_mae: 0.7417\n",
      "Epoch 80/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.9033 - mae: 0.6610 - val_loss: 1.1529 - val_mae: 0.7417\n",
      "Epoch 81/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 0.8954 - mae: 0.6617 - val_loss: 1.1527 - val_mae: 0.7417\n",
      "Epoch 82/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.9029 - mae: 0.6618 - val_loss: 1.1535 - val_mae: 0.7421\n",
      "Epoch 83/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - loss: 0.8994 - mae: 0.6634 - val_loss: 1.1537 - val_mae: 0.7422\n",
      "Epoch 84/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.8959 - mae: 0.6594 - val_loss: 1.1531 - val_mae: 0.7421\n",
      "Epoch 85/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.8866 - mae: 0.6586 - val_loss: 1.1529 - val_mae: 0.7422\n",
      "Epoch 86/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 0.8943 - mae: 0.6608 - val_loss: 1.1524 - val_mae: 0.7419\n",
      "Epoch 87/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.8864 - mae: 0.6587 - val_loss: 1.1523 - val_mae: 0.7422\n",
      "Epoch 88/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.8920 - mae: 0.6608 - val_loss: 1.1522 - val_mae: 0.7420\n",
      "Epoch 89/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - loss: 0.8965 - mae: 0.6603 - val_loss: 1.1521 - val_mae: 0.7420\n",
      "Epoch 90/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.8815 - mae: 0.6566 - val_loss: 1.1523 - val_mae: 0.7422\n",
      "Epoch 91/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 0.8822 - mae: 0.6569 - val_loss: 1.1532 - val_mae: 0.7425\n",
      "Epoch 92/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.8798 - mae: 0.6580 - val_loss: 1.1532 - val_mae: 0.7427\n",
      "Epoch 93/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.8842 - mae: 0.6581 - val_loss: 1.1533 - val_mae: 0.7430\n",
      "Epoch 94/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.8879 - mae: 0.6562 - val_loss: 1.1532 - val_mae: 0.7431\n",
      "Epoch 95/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495us/step - loss: 0.8845 - mae: 0.6569 - val_loss: 1.1533 - val_mae: 0.7433\n",
      "Epoch 96/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.8787 - mae: 0.6542 - val_loss: 1.1537 - val_mae: 0.7435\n",
      "Epoch 97/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.8777 - mae: 0.6558 - val_loss: 1.1537 - val_mae: 0.7437\n",
      "Epoch 98/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - loss: 0.8777 - mae: 0.6530 - val_loss: 1.1543 - val_mae: 0.7442\n",
      "Epoch 99/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 0.8805 - mae: 0.6549 - val_loss: 1.1536 - val_mae: 0.7437\n",
      "Epoch 100/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.8810 - mae: 0.6581 - val_loss: 1.1542 - val_mae: 0.7440\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(in_dim, hidden=(64, 32), dropout=0.1, lr=1e-3):\n",
    "    inputs = keras.Input(shape=(in_dim,))\n",
    "    x = inputs\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation=\"relu\",)(x) #kernel_regularizer=regularizers.l2(1e-4)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_mlp(in_dim=X_tr.shape[1], hidden=(32, 16), dropout=0.1, lr=1e-4)\n",
    "# model = build_mlp(in_dim=X_tr.shape[1], hidden=(15, 15, 15, 15, 15, 15), dropout=0.0, lr=1e-4)\n",
    "# model = build_mlp(in_dim=X_tr.shape[1], hidden=(15, 15, 15, 15, 15, 15), dropout=0.0, lr=0.01)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    # X_tr, y_tr,\n",
    "    X_tr, y_tr_s,   ## Scaled y\n",
    "    # validation_data=(X_va, y_va),\n",
    "    validation_data=(X_va, y_va_s),   ## Scaled y\n",
    "    # epochs=150,\n",
    "    epochs=100,\n",
    "    # batch_size=256,\n",
    "    batch_size=100,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "55b4068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def prediction_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    sign_acc = (np.sign(y_true) == np.sign(y_pred)).mean()\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan\n",
    "    \n",
    "    return {\"MSE\": round(mse, 5), \"MAE\": round(mae, 5), \"R2\": round(r2, 5), \"SignAcc\": round(float(sign_acc), 5), \"Corr\": round(float(corr), 5)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "841800ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean\n",
      "{'MSE': 0.00049, 'MAE': 0.01368, 'R2': -0.05323, 'SignAcc': 0.48295, 'Corr': -0.08989}\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "MLP\n",
      "{'MSE': 0.00049, 'MAE': 0.01385, 'R2': -0.04723, 'SignAcc': 0.49535, 'Corr': 0.00419}\n",
      "0.0022442322\n"
     ]
    }
   ],
   "source": [
    "y_pred_sm = X_past_returns_test_raw.astype(np.float32).mean(axis=1)\n",
    "# X_te_raw = scaler.inverse_transform(X_te)\n",
    "# y_pred_sm = X_te_raw.mean(axis=1)\n",
    "print(\"Sample Mean\")\n",
    "print(prediction_metrics(y_te, y_pred_sm))\n",
    "\n",
    "# y_pred = model.predict(X_te, batch_size=1024).squeeze()\n",
    "## Scaled y:\n",
    "pred_s = model.predict(X_te, batch_size=1024).squeeze()\n",
    "y_pred = y_mean + y_std * pred_s\n",
    "\n",
    "print(\"MLP\")\n",
    "print(prediction_metrics(y_te, y_pred))\n",
    "\n",
    "print(y_te.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "75af9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_sample_mean(test_window):\n",
    "    # past_weekly_returns: (lookback periods) x (assets)\n",
    "    return test_window[\"past_weekly_returns\"].mean(axis=0)  # pd.Series indexed by asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35858728",
   "metadata": {},
   "source": [
    "Walk-Forward Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "f2307a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_eval_mlp(\n",
    "    rolling,\n",
    "    train_len=150,          # number of windows to train on each step\n",
    "    x_key=\"X_feat\",\n",
    "    y_key=\"y_ret\",\n",
    "    hidden=(64, 32),\n",
    "    dropout=0.1,\n",
    "    lr=3e-4,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    seed=42\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    all_rows = []\n",
    "    week_metrics = []\n",
    "\n",
    "    for i in range(train_len, len(rolling)):\n",
    "        train_windows = rolling[i-train_len:i]\n",
    "        test_window   = rolling[i]\n",
    "\n",
    "        # --- build train panel ---\n",
    "        X_train, y_train, meta = panel_from_windows(train_windows, x_key=x_key, y_key=y_key)\n",
    "        if X_train.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # --- scaler on TRAIN only ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "        # --- build test cross-section ---\n",
    "        X_test_df = test_window[x_key]            # assets x features\n",
    "        y_test = test_window[y_key]\n",
    "        if not isinstance(y_test, pd.Series):\n",
    "            y_test = pd.Series(y_test, index=X_test_df.index)\n",
    "\n",
    "        assets = X_test_df.index.intersection(y_test.index).sort_values()\n",
    "        X_test = X_test_df.loc[assets].to_numpy(np.float32)\n",
    "        y_true = y_test.loc[assets].to_numpy(np.float32)\n",
    "\n",
    "        mask = np.isfinite(X_test).all(axis=1) & np.isfinite(y_true)\n",
    "        assets = assets[mask]\n",
    "        X_test = X_test[mask]\n",
    "        y_true = y_true[mask]\n",
    "\n",
    "        X_test_sc = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "        # --- train model (fresh each step) ---\n",
    "        model = build_mlp(in_dim=X_train_sc.shape[1], hidden=hidden, dropout=dropout, lr=lr)\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_sc, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "\n",
    "        # --- predict ---\n",
    "        y_pred = model.predict(X_test_sc, batch_size=1024, verbose=0).squeeze()\n",
    "\n",
    "        # --- baseline: sample mean of past period returns ---\n",
    "        y_pred_sm_ser = baseline_sample_mean(test_window).loc[assets]\n",
    "        y_pred_sm = y_pred_sm_ser.to_numpy(np.float32)\n",
    "\n",
    "        # store per-asset predictions\n",
    "        for a, yt, yp, ypsm in zip(assets, y_true, y_pred, y_pred_sm):\n",
    "            all_rows.append(\n",
    "                {\"window_idx\": i, \"asset\": a, \"y\": float(yt), \"pred_mlp\": float(yp), \"pred_sm\": float(ypsm)}\n",
    "            )\n",
    "\n",
    "        # per-window metrics (cross-section)\n",
    "        m_mlp = prediction_metrics(y_true, y_pred)\n",
    "        m_sm  = prediction_metrics(y_true, y_pred_sm)\n",
    "        week_metrics.append({\"window_idx\": i, **{f\"mlp_{k}\": v for k,v in m_mlp.items()},\n",
    "                                **{f\"sm_{k}\": v for k,v in m_sm.items()}})\n",
    "\n",
    "    preds_df = pd.DataFrame(all_rows)\n",
    "    week_df  = pd.DataFrame(week_metrics)\n",
    "\n",
    "    # pooled metrics over all (window, asset) test points\n",
    "    pooled_mlp = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_mlp\"])\n",
    "    pooled_sm  = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_sm\"])\n",
    "\n",
    "    return preds_df, week_df, pooled_mlp, pooled_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "394ebbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_df, week_df, pooled_mlp, pooled_sm = walk_forward_eval_mlp(\n",
    "#     rolling=rolling,\n",
    "#     train_len=100,   # e.g., last 52 periods (with days_per_week=2 that's ~104 trading days)\n",
    "#     x_key=\"X_feat\",\n",
    "#     y_key=\"y_ret\",\n",
    "#     hidden=(16,8)\n",
    "# )\n",
    "\n",
    "# print(\"Pooled MLP:\", pooled_mlp)\n",
    "# print(\"Pooled  SM:\", pooled_sm)\n",
    "# print(\"Weekly win-rate (MAE):\", (week_df[\"mlp_MAE\"] < week_df[\"sm_MAE\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "3f9a76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_export_mlp(\n",
    "    rolling,\n",
    "    markets: list[str],\n",
    "    model_name: str = \"MLP\",\n",
    "    year: int = 2025,\n",
    "    train_len: int = 200,\n",
    "    x_key: str = \"past_weekly_returns\",\n",
    "    y_key: str = \"y_ret\",\n",
    "    hidden=(32, 16),\n",
    "    dropout: float = 0.1,\n",
    "    lr: float = 3e-4,\n",
    "    epochs: int = 150,\n",
    "    batch_size: int = 256,\n",
    "    seed: int = 42,\n",
    "    out_dir: str = \"data/prediction\",\n",
    "    date_key_candidates=(\"t1\", \"t0\", \"date\", \"Date\"),\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    markets_str = \"-\".join(markets)\n",
    "\n",
    "    rows_pred = []\n",
    "    rows_true = []\n",
    "\n",
    "    def get_window_date(w):\n",
    "        for k in date_key_candidates:\n",
    "            if k in w:\n",
    "                d = pd.to_datetime(w[k])\n",
    "                return d\n",
    "        return None  # if no date info\n",
    "\n",
    "    # find first index in 2025 (so we \"start walking forward on 2025\")\n",
    "    first_2025_idx = None\n",
    "    for i in range(len(rolling)):\n",
    "        d = get_window_date(rolling[i])\n",
    "        if isinstance(d, pd.Timestamp) and d.year == year:\n",
    "            first_2025_idx = i\n",
    "            break\n",
    "\n",
    "    if first_2025_idx is None:\n",
    "        raise ValueError(f\"No windows found with year={year}. Check your rolling windows date keys.\")\n",
    "\n",
    "    # walk-forward loop: start at first 2025 window, but still need train_len history\n",
    "    start_i = max(train_len, first_2025_idx)\n",
    "    print(\"START i \")\n",
    "    print(first_2025_idx)\n",
    "    print(start_i)\n",
    "    print(len(rolling))\n",
    "    for i in range(start_i, len(rolling)):\n",
    "        train_windows = rolling[i-train_len:i]\n",
    "        test_window   = rolling[i]\n",
    "\n",
    "        period = get_window_date(test_window)\n",
    "        if isinstance(period, pd.Timestamp):\n",
    "            if period.year != year:\n",
    "                continue\n",
    "        else:\n",
    "            # if no dates, we cannot filter by year; safest is to skip\n",
    "            continue\n",
    "\n",
    "        # --- build train panel ---\n",
    "        X_train, y_train, _ = panel_from_windows(train_windows, x_key=x_key, y_key=y_key)\n",
    "        if X_train.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # --- scaler on TRAIN only ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "        # --- build test cross-section ---\n",
    "        X_test_df = test_window[x_key].T  # assets x features\n",
    "        y_test = test_window[y_key]\n",
    "        if not isinstance(y_test, pd.Series):\n",
    "            y_test = pd.Series(y_test, index=X_test_df.index)\n",
    "\n",
    "        assets = X_test_df.index.intersection(y_test.index).sort_values()\n",
    "        if len(assets) == 0:\n",
    "            continue\n",
    "\n",
    "        X_test = X_test_df.loc[assets].to_numpy(np.float32)\n",
    "        y_true = y_test.loc[assets].to_numpy(np.float32)\n",
    "\n",
    "        mask = np.isfinite(X_test).all(axis=1) & np.isfinite(y_true)\n",
    "        assets = assets[mask]\n",
    "        X_test = X_test[mask]\n",
    "        y_true = y_true[mask]\n",
    "\n",
    "        if len(assets) == 0:\n",
    "            continue\n",
    "\n",
    "        X_test_sc = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "        # --- train model (fresh each step) ---\n",
    "        keras.backend.clear_session()\n",
    "        model = build_mlp(in_dim=X_train_sc.shape[1], hidden=hidden, dropout=dropout, lr=lr)\n",
    "\n",
    "        # (optional) early stopping; monitor val_loss only if you pass validation_data\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_sc, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "\n",
    "        # --- predict ---\n",
    "        y_pred = model.predict(X_test_sc, batch_size=1024, verbose=0).squeeze()\n",
    "\n",
    "        # store long rows (period, asset)\n",
    "        for a, yt, yp in zip(assets, y_true, y_pred):\n",
    "            rows_true.append({\"period\": period, \"asset\": a, \"true\": float(yt)})\n",
    "            rows_pred.append({\"period\": period, \"asset\": a, \"pred\": float(yp)})\n",
    "\n",
    "    # --- long -> wide ---\n",
    "    pred_long = pd.DataFrame(rows_pred)\n",
    "    true_long = pd.DataFrame(rows_true)\n",
    "\n",
    "    if pred_long.empty:\n",
    "        raise ValueError(\"No predictions were produced. Check train_len, date keys, or rolling coverage.\")\n",
    "\n",
    "    expected_df = pred_long.pivot(index=\"period\", columns=\"asset\", values=\"pred\").sort_index()\n",
    "    true_df     = true_long.pivot(index=\"period\", columns=\"asset\", values=\"true\").sort_index()\n",
    "\n",
    "    # align columns (union) so shapes match\n",
    "    all_cols = expected_df.columns.union(true_df.columns)\n",
    "    expected_df = expected_df.reindex(columns=all_cols)\n",
    "    true_df     = true_df.reindex(columns=all_cols)\n",
    "\n",
    "    errors_df = true_df - expected_df  # error = true - pred\n",
    "\n",
    "    # --- save ---\n",
    "    p_exp = os.path.join(out_dir, f\"{model_name}_{markets_str}_expected_returns.csv\")\n",
    "    p_true = os.path.join(out_dir, f\"{model_name}_{markets_str}_true_returns.csv\")\n",
    "    p_err = os.path.join(out_dir, f\"{model_name}_{markets_str}_errors.csv\")\n",
    "\n",
    "    expected_df.to_csv(p_exp)\n",
    "    true_df.to_csv(p_true)\n",
    "    errors_df.to_csv(p_err)\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(\" \", p_exp, expected_df.shape)\n",
    "    print(\" \", p_true, true_df.shape)\n",
    "    print(\" \", p_err, errors_df.shape)\n",
    "\n",
    "    return expected_df, true_df, errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "b2048a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START i \n",
      "316\n",
      "316\n",
      "430\n",
      "Saved:\n",
      "  data/prediction/MLP_dow30_expected_returns.csv (114, 30)\n",
      "  data/prediction/MLP_dow30_true_returns.csv (114, 30)\n",
      "  data/prediction/MLP_dow30_errors.csv (114, 30)\n"
     ]
    }
   ],
   "source": [
    "expected_2025, true_2025, errors_2025 = walk_forward_export_mlp(\n",
    "    rolling=rolling,\n",
    "    markets=markets,     \n",
    "    model_name=\"MLP\",\n",
    "    year=2025,\n",
    "    train_len=200,\n",
    "    x_key=\"past_weekly_returns\",\n",
    "    y_key=\"y_ret\",\n",
    "    hidden=(32, 16),\n",
    "    dropout=0.1,\n",
    "    lr=1e-4,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    out_dir=\"data/prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea1254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
