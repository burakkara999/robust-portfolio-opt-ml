{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48d5911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from asset_data_module import read_close_prices_all_merged\n",
    "from features import make_feature_windows\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1d2ad77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((375, 30), 137)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markets = ['dow30']\n",
    "# markets = ['commodities']\n",
    "# start_date, end_date = \"2022-01-01\", \"2025-11-28\"\n",
    "start_date, end_date = \"2024-06-01\", \"2025-11-28\"\n",
    "\n",
    "_, close_df = read_close_prices_all_merged(markets, after_date=start_date)\n",
    "close_df = close_df.loc[:end_date]\n",
    "\n",
    "rolling = make_feature_windows(\n",
    "    close_prices=close_df,\n",
    "    lookback=50,\n",
    "    horizon=1,\n",
    "    days_per_week=2\n",
    ")\n",
    "close_df.shape, len(rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22a499ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow30:AAPL</th>\n",
       "      <th>dow30:AMGN</th>\n",
       "      <th>dow30:AXP</th>\n",
       "      <th>dow30:BA</th>\n",
       "      <th>dow30:CAT</th>\n",
       "      <th>dow30:CRM</th>\n",
       "      <th>dow30:CSCO</th>\n",
       "      <th>dow30:CVX</th>\n",
       "      <th>dow30:DIS</th>\n",
       "      <th>dow30:DOW</th>\n",
       "      <th>...</th>\n",
       "      <th>dow30:MSFT</th>\n",
       "      <th>dow30:NKE</th>\n",
       "      <th>dow30:PFE</th>\n",
       "      <th>dow30:PG</th>\n",
       "      <th>dow30:RTX</th>\n",
       "      <th>dow30:TRV</th>\n",
       "      <th>dow30:UNH</th>\n",
       "      <th>dow30:V</th>\n",
       "      <th>dow30:VZ</th>\n",
       "      <th>dow30:WMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009438</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.009288</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>-0.005781</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>-0.013597</td>\n",
       "      <td>-0.015876</td>\n",
       "      <td>-0.012435</td>\n",
       "      <td>-0.006800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>-0.000848</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.008467</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>-0.009218</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>0.015123</td>\n",
       "      <td>0.008988</td>\n",
       "      <td>0.019111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005194</td>\n",
       "      <td>-0.007707</td>\n",
       "      <td>-0.008644</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.022243</td>\n",
       "      <td>-0.003919</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000378</td>\n",
       "      <td>0.023368</td>\n",
       "      <td>-0.033377</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-0.003048</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>-0.025016</td>\n",
       "      <td>0.015077</td>\n",
       "      <td>-0.009965</td>\n",
       "      <td>-0.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050798</td>\n",
       "      <td>-0.014264</td>\n",
       "      <td>-0.035612</td>\n",
       "      <td>-0.025232</td>\n",
       "      <td>-0.004968</td>\n",
       "      <td>-0.003562</td>\n",
       "      <td>-0.001528</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>-0.006620</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>-0.007068</td>\n",
       "      <td>-0.019432</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>-0.011068</td>\n",
       "      <td>-0.011820</td>\n",
       "      <td>0.011207</td>\n",
       "      <td>-0.014458</td>\n",
       "      <td>-0.012783</td>\n",
       "      <td>0.012820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033654</td>\n",
       "      <td>-0.007343</td>\n",
       "      <td>-0.010476</td>\n",
       "      <td>-0.026217</td>\n",
       "      <td>-0.002907</td>\n",
       "      <td>-0.050903</td>\n",
       "      <td>-0.005477</td>\n",
       "      <td>-0.023601</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>-0.001604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>-0.017785</td>\n",
       "      <td>-0.013650</td>\n",
       "      <td>-0.006409</td>\n",
       "      <td>-0.017835</td>\n",
       "      <td>-0.011237</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>-0.012751</td>\n",
       "      <td>-0.015960</td>\n",
       "      <td>-0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011278</td>\n",
       "      <td>0.015887</td>\n",
       "      <td>0.026996</td>\n",
       "      <td>-0.012866</td>\n",
       "      <td>-0.012208</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.014186</td>\n",
       "      <td>-0.014916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>0.008669</td>\n",
       "      <td>-0.024530</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>-0.012127</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.008077</td>\n",
       "      <td>0.010737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dow30:AAPL  dow30:AMGN  dow30:AXP  dow30:BA  dow30:CAT  dow30:CRM  \\\n",
       "0    0.009438   -0.000130  -0.009288  0.028205  -0.005781  -0.000380   \n",
       "1    0.005194   -0.007707  -0.008644  0.002052  -0.001549   0.022243   \n",
       "2    0.050798   -0.014264  -0.035612 -0.025232  -0.004968  -0.003562   \n",
       "3    0.033654   -0.007343  -0.010476 -0.026217  -0.002907  -0.050903   \n",
       "4    0.011278    0.015887   0.026996 -0.012866  -0.012208   0.006311   \n",
       "\n",
       "   dow30:CSCO  dow30:CVX  dow30:DIS  dow30:DOW  ...  dow30:MSFT  dow30:NKE  \\\n",
       "0   -0.013597  -0.015876  -0.012435  -0.006800  ...    0.025051  -0.000848   \n",
       "1   -0.003919   0.007073   0.000394   0.000718  ...   -0.000378   0.023368   \n",
       "2   -0.001528   0.002623  -0.006620   0.007508  ...    0.020619  -0.007068   \n",
       "3   -0.005477  -0.023601  -0.007763  -0.001604  ...    0.020361  -0.017785   \n",
       "4    0.003728   0.002287   0.014186  -0.014916  ...    0.015259   0.008669   \n",
       "\n",
       "   dow30:PFE  dow30:PG  dow30:RTX  dow30:TRV  dow30:UNH   dow30:V  dow30:VZ  \\\n",
       "0   0.008155  0.008467   0.009916  -0.009218   0.011354  0.015123  0.008988   \n",
       "1  -0.033377  0.006064  -0.003048   0.021550  -0.025016  0.015077 -0.009965   \n",
       "2  -0.019432  0.002511  -0.011068  -0.011820   0.011207 -0.014458 -0.012783   \n",
       "3  -0.013650 -0.006409  -0.017835  -0.011237   0.002174 -0.012751 -0.015960   \n",
       "4  -0.024530  0.006528  -0.005346   0.005106  -0.012127 -0.000074 -0.008077   \n",
       "\n",
       "   dow30:WMT  \n",
       "0   0.019111  \n",
       "1  -0.018200  \n",
       "2   0.012820  \n",
       "3  -0.000450  \n",
       "4   0.010737  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rolling[0]['past_weekly_returns'].shape)\n",
    "rolling[0]['past_weekly_returns'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09518f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom_1w</th>\n",
       "      <th>mom_4w</th>\n",
       "      <th>mom_12w</th>\n",
       "      <th>vol_1w</th>\n",
       "      <th>vol_4w</th>\n",
       "      <th>sharpe_1w</th>\n",
       "      <th>sharpe_4w</th>\n",
       "      <th>vol_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dow30:AAPL</th>\n",
       "      <td>-0.815335</td>\n",
       "      <td>0.571021</td>\n",
       "      <td>-0.150967</td>\n",
       "      <td>0.379078</td>\n",
       "      <td>-0.053702</td>\n",
       "      <td>-0.187871</td>\n",
       "      <td>0.585219</td>\n",
       "      <td>0.348134</td>\n",
       "      <td>0.014406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:AMGN</th>\n",
       "      <td>0.252263</td>\n",
       "      <td>-1.075911</td>\n",
       "      <td>-1.667085</td>\n",
       "      <td>0.665689</td>\n",
       "      <td>-0.565586</td>\n",
       "      <td>-0.181922</td>\n",
       "      <td>-1.614424</td>\n",
       "      <td>0.414970</td>\n",
       "      <td>0.563006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:AXP</th>\n",
       "      <td>0.689067</td>\n",
       "      <td>-0.373531</td>\n",
       "      <td>-0.076728</td>\n",
       "      <td>-0.132065</td>\n",
       "      <td>0.554010</td>\n",
       "      <td>-0.179532</td>\n",
       "      <td>-0.252939</td>\n",
       "      <td>-0.191048</td>\n",
       "      <td>0.140703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:BA</th>\n",
       "      <td>-0.411102</td>\n",
       "      <td>1.314572</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.317401</td>\n",
       "      <td>0.578818</td>\n",
       "      <td>-0.186059</td>\n",
       "      <td>1.045019</td>\n",
       "      <td>-0.045177</td>\n",
       "      <td>-1.412233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow30:CAT</th>\n",
       "      <td>-0.114660</td>\n",
       "      <td>-0.956522</td>\n",
       "      <td>0.432995</td>\n",
       "      <td>-0.627754</td>\n",
       "      <td>-0.351098</td>\n",
       "      <td>-0.218349</td>\n",
       "      <td>-1.217257</td>\n",
       "      <td>-0.679821</td>\n",
       "      <td>0.010206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mom_1w    mom_4w   mom_12w    vol_1w    vol_4w  sharpe_1w  \\\n",
       "dow30:AAPL -0.815335  0.571021 -0.150967  0.379078 -0.053702  -0.187871   \n",
       "dow30:AMGN  0.252263 -1.075911 -1.667085  0.665689 -0.565586  -0.181922   \n",
       "dow30:AXP   0.689067 -0.373531 -0.076728 -0.132065  0.554010  -0.179532   \n",
       "dow30:BA   -0.411102  1.314572  0.007595  0.317401  0.578818  -0.186059   \n",
       "dow30:CAT  -0.114660 -0.956522  0.432995 -0.627754 -0.351098  -0.218349   \n",
       "\n",
       "            sharpe_4w  vol_ratio  max_drawdown  \n",
       "dow30:AAPL   0.585219   0.348134      0.014406  \n",
       "dow30:AMGN  -1.614424   0.414970      0.563006  \n",
       "dow30:AXP   -0.252939  -0.191048      0.140703  \n",
       "dow30:BA     1.045019  -0.045177     -1.412233  \n",
       "dow30:CAT   -1.217257  -0.679821      0.010206  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['X_feat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "961e94f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dow30:AAPL    0.003637\n",
       "dow30:AMGN    0.004078\n",
       "dow30:AXP    -0.009789\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['y_ret'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d2947a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_from_windows(windows, x_key=\"past_weekly_returns\", y_key=\"y_ret\"):\n",
    "    X_list, y_list = [], []\n",
    "    meta_rows = []\n",
    "\n",
    "    for w_idx, w in enumerate(windows):\n",
    "        if x_key == 'past_weekly_returns':\n",
    "            X_df = w[x_key].T          # assets x n_lookback (weeks) (DataFrame)\n",
    "        elif x_key == 'X_feat':\n",
    "            X_df = w[x_key]          # assets x n_features (DataFrame)\n",
    "        y_ser = w[y_key]           # assets (Series or array)\n",
    "\n",
    "        if not isinstance(y_ser, pd.Series):\n",
    "            y_ser = pd.Series(y_ser, index=X_df.index)\n",
    "\n",
    "        assets = X_df.index.intersection(y_ser.index)\n",
    "        Xw = X_df.loc[assets].to_numpy(dtype=np.float32)\n",
    "        yw = y_ser.loc[assets].to_numpy(dtype=np.float32)\n",
    "\n",
    "        mask = np.isfinite(Xw).all(axis=1) & np.isfinite(yw)\n",
    "        Xw, yw = Xw[mask], yw[mask]\n",
    "        assets_kept = assets.to_numpy()[mask]\n",
    "\n",
    "        X_list.append(Xw)\n",
    "        y_list.append(yw)\n",
    "\n",
    "        t0, t1 = w.get(\"t0\", None), w.get(\"t1\", None)\n",
    "        for a in assets_kept:\n",
    "            meta_rows.append((w_idx, a, t0, t1))\n",
    "\n",
    "    X = np.vstack(X_list) ## weeks*assets x n_lookback/n_features\n",
    "    y = np.concatenate(y_list)\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"window_idx\", \"asset\", \"t0\", \"t1\"])\n",
    "    return X, y, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00e8a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4110, 9),\n",
       " (4110,),\n",
       "    window_idx       asset         t0         t1\n",
       " 0           0  dow30:AAPL 2024-10-23 2024-10-25\n",
       " 1           0  dow30:AMGN 2024-10-23 2024-10-25\n",
       " 2           0   dow30:AXP 2024-10-23 2024-10-25\n",
       " 3           0    dow30:BA 2024-10-23 2024-10-25\n",
       " 4           0   dow30:CAT 2024-10-23 2024-10-25)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_past_returns, y, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "X, y, meta = panel_from_windows(rolling, x_key='X_feat')\n",
    "X.shape, y.shape, meta.head() ## len(rolling)*n_asset -- each row is a feature set -- to predict y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dcd561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = rolling  # your rolling list\n",
    "\n",
    "W = meta[\"window_idx\"].nunique()\n",
    "split_w = int(0.8 * W)\n",
    "\n",
    "train_mask = (meta[\"window_idx\"] < split_w).values\n",
    "test_mask  = (meta[\"window_idx\"] >= split_w).values\n",
    "\n",
    "X_train_raw, y_train = X[train_mask], y[train_mask]\n",
    "X_test_raw,  y_test  = X[test_mask],  y[test_mask]\n",
    "X_past_returns_test_raw = X_past_returns[test_mask]\n",
    "\n",
    "# small validation from the tail of the training windows\n",
    "val_w = max(int(0.1 * split_w), 1)\n",
    "val_start = split_w - val_w\n",
    "val_mask = ((meta[\"window_idx\"] >= val_start) & (meta[\"window_idx\"] < split_w)).values\n",
    "tr2_mask = (meta[\"window_idx\"] < val_start).values\n",
    "\n",
    "X_tr_raw, y_tr = X[tr2_mask], y[tr2_mask]\n",
    "X_va_raw, y_va = X[val_mask], y[val_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr_raw).astype(np.float32)\n",
    "X_va = scaler.transform(X_va_raw).astype(np.float32)\n",
    "X_te = scaler.transform(X_test_raw).astype(np.float32)\n",
    "\n",
    "y_tr = y_tr.astype(np.float32)\n",
    "y_va = y_va.astype(np.float32)\n",
    "y_te = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0b49e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2970, 9), (300, 9), (840, 9), (2970,), (300,), (840,), '28.0 test periods')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_va.shape, X_te.shape, y_tr.shape, y_va.shape, y_te.shape, f\"{y_te.shape[0]/close_df.shape[1]} test periods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0569f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.2811 - mae: 0.3932 - val_loss: 0.1627 - val_mae: 0.2833\n",
      "Epoch 2/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2376 - mae: 0.3566 - val_loss: 0.1364 - val_mae: 0.2561\n",
      "Epoch 3/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2060 - mae: 0.3266 - val_loss: 0.1166 - val_mae: 0.2350\n",
      "Epoch 4/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1873 - mae: 0.3098 - val_loss: 0.1013 - val_mae: 0.2176\n",
      "Epoch 5/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1499 - mae: 0.2833 - val_loss: 0.0889 - val_mae: 0.2037\n",
      "Epoch 6/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1426 - mae: 0.2677 - val_loss: 0.0790 - val_mae: 0.1927\n",
      "Epoch 7/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1275 - mae: 0.2546 - val_loss: 0.0707 - val_mae: 0.1832\n",
      "Epoch 8/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1214 - mae: 0.2447 - val_loss: 0.0640 - val_mae: 0.1752\n",
      "Epoch 9/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1080 - mae: 0.2308 - val_loss: 0.0583 - val_mae: 0.1682\n",
      "Epoch 10/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0971 - mae: 0.2212 - val_loss: 0.0534 - val_mae: 0.1619\n",
      "Epoch 11/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0901 - mae: 0.2108 - val_loss: 0.0493 - val_mae: 0.1563\n",
      "Epoch 12/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0885 - mae: 0.2047 - val_loss: 0.0457 - val_mae: 0.1512\n",
      "Epoch 13/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0794 - mae: 0.1964 - val_loss: 0.0422 - val_mae: 0.1462\n",
      "Epoch 14/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0706 - mae: 0.1862 - val_loss: 0.0391 - val_mae: 0.1413\n",
      "Epoch 15/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0675 - mae: 0.1795 - val_loss: 0.0367 - val_mae: 0.1374\n",
      "Epoch 16/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0659 - mae: 0.1782 - val_loss: 0.0345 - val_mae: 0.1337\n",
      "Epoch 17/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0607 - mae: 0.1674 - val_loss: 0.0324 - val_mae: 0.1299\n",
      "Epoch 18/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0543 - mae: 0.1640 - val_loss: 0.0303 - val_mae: 0.1258\n",
      "Epoch 19/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0552 - mae: 0.1611 - val_loss: 0.0284 - val_mae: 0.1218\n",
      "Epoch 20/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0525 - mae: 0.1560 - val_loss: 0.0266 - val_mae: 0.1182\n",
      "Epoch 21/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499 - mae: 0.1524 - val_loss: 0.0250 - val_mae: 0.1146\n",
      "Epoch 22/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0450 - mae: 0.1476 - val_loss: 0.0235 - val_mae: 0.1110\n",
      "Epoch 23/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0415 - mae: 0.1409 - val_loss: 0.0221 - val_mae: 0.1075\n",
      "Epoch 24/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1425 - val_loss: 0.0207 - val_mae: 0.1040\n",
      "Epoch 25/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0396 - mae: 0.1366 - val_loss: 0.0196 - val_mae: 0.1008\n",
      "Epoch 26/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0380 - mae: 0.1325 - val_loss: 0.0185 - val_mae: 0.0980\n",
      "Epoch 27/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0357 - mae: 0.1293 - val_loss: 0.0174 - val_mae: 0.0951\n",
      "Epoch 28/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0326 - mae: 0.1229 - val_loss: 0.0165 - val_mae: 0.0927\n",
      "Epoch 29/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0308 - mae: 0.1200 - val_loss: 0.0158 - val_mae: 0.0907\n",
      "Epoch 30/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0308 - mae: 0.1208 - val_loss: 0.0150 - val_mae: 0.0884\n",
      "Epoch 31/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0311 - mae: 0.1183 - val_loss: 0.0142 - val_mae: 0.0858\n",
      "Epoch 32/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0305 - mae: 0.1172 - val_loss: 0.0134 - val_mae: 0.0834\n",
      "Epoch 33/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0253 - mae: 0.1103 - val_loss: 0.0127 - val_mae: 0.0810\n",
      "Epoch 34/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0267 - mae: 0.1095 - val_loss: 0.0120 - val_mae: 0.0789\n",
      "Epoch 35/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0250 - mae: 0.1057 - val_loss: 0.0115 - val_mae: 0.0769\n",
      "Epoch 36/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0237 - mae: 0.1043 - val_loss: 0.0109 - val_mae: 0.0748\n",
      "Epoch 37/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0216 - mae: 0.0986 - val_loss: 0.0103 - val_mae: 0.0729\n",
      "Epoch 38/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0226 - mae: 0.1001 - val_loss: 0.0098 - val_mae: 0.0714\n",
      "Epoch 39/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0208 - mae: 0.0970 - val_loss: 0.0092 - val_mae: 0.0697\n",
      "Epoch 40/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0192 - mae: 0.0951 - val_loss: 0.0088 - val_mae: 0.0681\n",
      "Epoch 41/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0182 - mae: 0.0923 - val_loss: 0.0083 - val_mae: 0.0668\n",
      "Epoch 42/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.0925 - val_loss: 0.0080 - val_mae: 0.0657\n",
      "Epoch 43/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0205 - mae: 0.0930 - val_loss: 0.0076 - val_mae: 0.0644\n",
      "Epoch 44/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mae: 0.0877 - val_loss: 0.0073 - val_mae: 0.0631\n",
      "Epoch 45/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0170 - mae: 0.0868 - val_loss: 0.0070 - val_mae: 0.0620\n",
      "Epoch 46/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0162 - mae: 0.0842 - val_loss: 0.0067 - val_mae: 0.0608\n",
      "Epoch 47/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0162 - mae: 0.0836 - val_loss: 0.0065 - val_mae: 0.0595\n",
      "Epoch 48/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0162 - mae: 0.0825 - val_loss: 0.0062 - val_mae: 0.0583\n",
      "Epoch 49/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0157 - mae: 0.0817 - val_loss: 0.0059 - val_mae: 0.0570\n",
      "Epoch 50/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0141 - mae: 0.0776 - val_loss: 0.0057 - val_mae: 0.0557\n",
      "Epoch 51/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0152 - mae: 0.0789 - val_loss: 0.0054 - val_mae: 0.0545\n",
      "Epoch 52/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0146 - mae: 0.0764 - val_loss: 0.0052 - val_mae: 0.0535\n",
      "Epoch 53/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0121 - mae: 0.0724 - val_loss: 0.0050 - val_mae: 0.0525\n",
      "Epoch 54/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0122 - mae: 0.0726 - val_loss: 0.0048 - val_mae: 0.0513\n",
      "Epoch 55/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 - mae: 0.0696 - val_loss: 0.0046 - val_mae: 0.0501\n",
      "Epoch 56/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0114 - mae: 0.0709 - val_loss: 0.0044 - val_mae: 0.0490\n",
      "Epoch 57/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0109 - mae: 0.0681 - val_loss: 0.0042 - val_mae: 0.0479\n",
      "Epoch 58/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0119 - mae: 0.0687 - val_loss: 0.0041 - val_mae: 0.0470\n",
      "Epoch 59/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0108 - mae: 0.0665 - val_loss: 0.0039 - val_mae: 0.0462\n",
      "Epoch 60/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0108 - mae: 0.0663 - val_loss: 0.0038 - val_mae: 0.0454\n",
      "Epoch 61/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0097 - mae: 0.0649 - val_loss: 0.0036 - val_mae: 0.0445\n",
      "Epoch 62/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0100 - mae: 0.0648 - val_loss: 0.0035 - val_mae: 0.0438\n",
      "Epoch 63/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0093 - mae: 0.0614 - val_loss: 0.0034 - val_mae: 0.0430\n",
      "Epoch 64/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0098 - mae: 0.0628 - val_loss: 0.0033 - val_mae: 0.0423\n",
      "Epoch 65/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0078 - mae: 0.0583 - val_loss: 0.0031 - val_mae: 0.0414\n",
      "Epoch 66/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0086 - mae: 0.0592 - val_loss: 0.0030 - val_mae: 0.0406\n",
      "Epoch 67/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0090 - mae: 0.0605 - val_loss: 0.0029 - val_mae: 0.0400\n",
      "Epoch 68/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0087 - mae: 0.0591 - val_loss: 0.0028 - val_mae: 0.0394\n",
      "Epoch 69/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0085 - mae: 0.0573 - val_loss: 0.0027 - val_mae: 0.0388\n",
      "Epoch 70/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0082 - mae: 0.0566 - val_loss: 0.0026 - val_mae: 0.0382\n",
      "Epoch 71/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0081 - mae: 0.0562 - val_loss: 0.0026 - val_mae: 0.0376\n",
      "Epoch 72/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0080 - mae: 0.0555 - val_loss: 0.0025 - val_mae: 0.0370\n",
      "Epoch 73/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0072 - mae: 0.0539 - val_loss: 0.0024 - val_mae: 0.0363\n",
      "Epoch 74/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0074 - mae: 0.0540 - val_loss: 0.0023 - val_mae: 0.0356\n",
      "Epoch 75/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0065 - mae: 0.0511 - val_loss: 0.0022 - val_mae: 0.0348\n",
      "Epoch 76/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0066 - mae: 0.0506 - val_loss: 0.0022 - val_mae: 0.0341\n",
      "Epoch 77/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0074 - mae: 0.0522 - val_loss: 0.0021 - val_mae: 0.0335\n",
      "Epoch 78/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0066 - mae: 0.0500 - val_loss: 0.0020 - val_mae: 0.0330\n",
      "Epoch 79/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0065 - mae: 0.0491 - val_loss: 0.0019 - val_mae: 0.0324\n",
      "Epoch 80/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0069 - mae: 0.0497 - val_loss: 0.0019 - val_mae: 0.0320\n",
      "Epoch 81/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0064 - mae: 0.0485 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 82/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0058 - mae: 0.0468 - val_loss: 0.0018 - val_mae: 0.0312\n",
      "Epoch 83/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0058 - mae: 0.0467 - val_loss: 0.0017 - val_mae: 0.0307\n",
      "Epoch 84/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0058 - mae: 0.0463 - val_loss: 0.0017 - val_mae: 0.0301\n",
      "Epoch 85/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0055 - mae: 0.0456 - val_loss: 0.0016 - val_mae: 0.0295\n",
      "Epoch 86/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0051 - mae: 0.0447 - val_loss: 0.0016 - val_mae: 0.0290\n",
      "Epoch 87/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0054 - mae: 0.0440 - val_loss: 0.0015 - val_mae: 0.0286\n",
      "Epoch 88/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0051 - mae: 0.0435 - val_loss: 0.0015 - val_mae: 0.0285\n",
      "Epoch 89/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0054 - mae: 0.0443 - val_loss: 0.0015 - val_mae: 0.0281\n",
      "Epoch 90/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0053 - mae: 0.0436 - val_loss: 0.0014 - val_mae: 0.0276\n",
      "Epoch 91/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0052 - mae: 0.0429 - val_loss: 0.0014 - val_mae: 0.0272\n",
      "Epoch 92/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0052 - mae: 0.0428 - val_loss: 0.0013 - val_mae: 0.0268\n",
      "Epoch 93/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0421 - val_loss: 0.0013 - val_mae: 0.0264\n",
      "Epoch 94/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0051 - mae: 0.0418 - val_loss: 0.0013 - val_mae: 0.0263\n",
      "Epoch 95/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0049 - mae: 0.0407 - val_loss: 0.0013 - val_mae: 0.0259\n",
      "Epoch 96/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0048 - mae: 0.0408 - val_loss: 0.0012 - val_mae: 0.0256\n",
      "Epoch 97/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0402 - val_loss: 0.0012 - val_mae: 0.0253\n",
      "Epoch 98/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - mae: 0.0392 - val_loss: 0.0012 - val_mae: 0.0250\n",
      "Epoch 99/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0403 - val_loss: 0.0011 - val_mae: 0.0246\n",
      "Epoch 100/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0048 - mae: 0.0390 - val_loss: 0.0011 - val_mae: 0.0243\n",
      "Epoch 101/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0049 - mae: 0.0387 - val_loss: 0.0011 - val_mae: 0.0239\n",
      "Epoch 102/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0045 - mae: 0.0386 - val_loss: 0.0011 - val_mae: 0.0236\n",
      "Epoch 103/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0043 - mae: 0.0373 - val_loss: 0.0010 - val_mae: 0.0233\n",
      "Epoch 104/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - mae: 0.0372 - val_loss: 0.0010 - val_mae: 0.0230\n",
      "Epoch 105/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0042 - mae: 0.0366 - val_loss: 9.8964e-04 - val_mae: 0.0227\n",
      "Epoch 106/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040 - mae: 0.0366 - val_loss: 9.6728e-04 - val_mae: 0.0224\n",
      "Epoch 107/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0045 - mae: 0.0371 - val_loss: 9.4505e-04 - val_mae: 0.0222\n",
      "Epoch 108/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0044 - mae: 0.0361 - val_loss: 9.2344e-04 - val_mae: 0.0219\n",
      "Epoch 109/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - mae: 0.0348 - val_loss: 9.0026e-04 - val_mae: 0.0216\n",
      "Epoch 110/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - mae: 0.0354 - val_loss: 8.8378e-04 - val_mae: 0.0214\n",
      "Epoch 111/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0035 - mae: 0.0346 - val_loss: 8.6653e-04 - val_mae: 0.0213\n",
      "Epoch 112/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0034 - mae: 0.0335 - val_loss: 8.4648e-04 - val_mae: 0.0210\n",
      "Epoch 113/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - mae: 0.0347 - val_loss: 8.2778e-04 - val_mae: 0.0207\n",
      "Epoch 114/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0034 - mae: 0.0337 - val_loss: 8.0997e-04 - val_mae: 0.0205\n",
      "Epoch 115/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0032 - mae: 0.0334 - val_loss: 7.9490e-04 - val_mae: 0.0202\n",
      "Epoch 116/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0031 - mae: 0.0327 - val_loss: 7.7988e-04 - val_mae: 0.0200\n",
      "Epoch 117/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0035 - mae: 0.0338 - val_loss: 7.6701e-04 - val_mae: 0.0199\n",
      "Epoch 118/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0315 - val_loss: 7.5330e-04 - val_mae: 0.0197\n",
      "Epoch 119/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0030 - mae: 0.0325 - val_loss: 7.4021e-04 - val_mae: 0.0195\n",
      "Epoch 120/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025 - mae: 0.0299 - val_loss: 7.2635e-04 - val_mae: 0.0193\n",
      "Epoch 121/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0034 - mae: 0.0308 - val_loss: 7.1165e-04 - val_mae: 0.0190\n",
      "Epoch 122/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025 - mae: 0.0299 - val_loss: 6.9936e-04 - val_mae: 0.0188\n",
      "Epoch 123/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0030 - mae: 0.0313 - val_loss: 6.8843e-04 - val_mae: 0.0186\n",
      "Epoch 124/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025 - mae: 0.0294 - val_loss: 6.7741e-04 - val_mae: 0.0185\n",
      "Epoch 125/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0299 - val_loss: 6.6580e-04 - val_mae: 0.0183\n",
      "Epoch 126/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0029 - mae: 0.0304 - val_loss: 6.5601e-04 - val_mae: 0.0182\n",
      "Epoch 127/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028 - mae: 0.0295 - val_loss: 6.4714e-04 - val_mae: 0.0181\n",
      "Epoch 128/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0299 - val_loss: 6.3616e-04 - val_mae: 0.0179\n",
      "Epoch 129/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0030 - mae: 0.0310 - val_loss: 6.2510e-04 - val_mae: 0.0177\n",
      "Epoch 130/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0287 - val_loss: 6.1495e-04 - val_mae: 0.0176\n",
      "Epoch 131/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024 - mae: 0.0283 - val_loss: 6.0459e-04 - val_mae: 0.0174\n",
      "Epoch 132/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022 - mae: 0.0279 - val_loss: 5.9386e-04 - val_mae: 0.0172\n",
      "Epoch 133/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023 - mae: 0.0278 - val_loss: 5.8267e-04 - val_mae: 0.0170\n",
      "Epoch 134/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025 - mae: 0.0286 - val_loss: 5.7281e-04 - val_mae: 0.0168\n",
      "Epoch 135/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0031 - mae: 0.0285 - val_loss: 5.6377e-04 - val_mae: 0.0167\n",
      "Epoch 136/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023 - mae: 0.0269 - val_loss: 5.5586e-04 - val_mae: 0.0165\n",
      "Epoch 137/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024 - mae: 0.0277 - val_loss: 5.4887e-04 - val_mae: 0.0164\n",
      "Epoch 138/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025 - mae: 0.0276 - val_loss: 5.4125e-04 - val_mae: 0.0163\n",
      "Epoch 139/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024 - mae: 0.0270 - val_loss: 5.3381e-04 - val_mae: 0.0161\n",
      "Epoch 140/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022 - mae: 0.0270 - val_loss: 5.2731e-04 - val_mae: 0.0160\n",
      "Epoch 141/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024 - mae: 0.0275 - val_loss: 5.2059e-04 - val_mae: 0.0159\n",
      "Epoch 142/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023 - mae: 0.0272 - val_loss: 5.1543e-04 - val_mae: 0.0158\n",
      "Epoch 143/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019 - mae: 0.0257 - val_loss: 5.0998e-04 - val_mae: 0.0158\n",
      "Epoch 144/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022 - mae: 0.0266 - val_loss: 5.0396e-04 - val_mae: 0.0157\n",
      "Epoch 145/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022 - mae: 0.0262 - val_loss: 4.9778e-04 - val_mae: 0.0155\n",
      "Epoch 146/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020 - mae: 0.0261 - val_loss: 4.9199e-04 - val_mae: 0.0154\n",
      "Epoch 147/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021 - mae: 0.0258 - val_loss: 4.8749e-04 - val_mae: 0.0154\n",
      "Epoch 148/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023 - mae: 0.0263 - val_loss: 4.8412e-04 - val_mae: 0.0153\n",
      "Epoch 149/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017 - mae: 0.0244 - val_loss: 4.7908e-04 - val_mae: 0.0152\n",
      "Epoch 150/150\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020 - mae: 0.0253 - val_loss: 4.7420e-04 - val_mae: 0.0151\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(in_dim, hidden=(64, 32), dropout=0.1, lr=1e-3):\n",
    "    inputs = keras.Input(shape=(in_dim,))\n",
    "    x = inputs\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_mlp(in_dim=X_tr.shape[1], hidden=(16, 8), dropout=0.1, lr=3e-4)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=20, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_va, y_va),\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55b4068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def prediction_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    sign_acc = (np.sign(y_true) == np.sign(y_pred)).mean()\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan\n",
    "    \n",
    "    return {\"MSE\": round(mse, 5), \"MAE\": round(mae, 5), \"R2\": round(r2, 5), \"SignAcc\": round(float(sign_acc), 5), \"Corr\": round(float(corr), 5)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "841800ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean\n",
      "{'MSE': 0.00029, 'MAE': 0.01161, 'R2': -0.0116, 'SignAcc': 0.53333, 'Corr': 0.06513}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "MLP\n",
      "{'MSE': 0.00051, 'MAE': 0.01567, 'R2': -0.75877, 'SignAcc': 0.54405, 'Corr': 0.03679}\n",
      "0.0013321199\n"
     ]
    }
   ],
   "source": [
    "y_pred_sm = X_past_returns_test_raw.astype(np.float32).mean(axis=1)\n",
    "# X_te_raw = scaler.inverse_transform(X_te)\n",
    "# y_pred_sm = X_te_raw.mean(axis=1)\n",
    "print(\"Sample Mean\")\n",
    "print(prediction_metrics(y_te, y_pred_sm))\n",
    "\n",
    "y_pred = model.predict(X_te, batch_size=1024).squeeze()\n",
    "print(\"MLP\")\n",
    "print(prediction_metrics(y_te, y_pred))\n",
    "\n",
    "print(y_te.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75af9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_sample_mean(test_window):\n",
    "    # past_weekly_returns: (lookback periods) x (assets)\n",
    "    return test_window[\"past_weekly_returns\"].mean(axis=0)  # pd.Series indexed by asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35858728",
   "metadata": {},
   "source": [
    "Walk-Forward Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2307a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_eval_mlp(\n",
    "    rolling,\n",
    "    train_len=150,          # number of windows to train on each step\n",
    "    x_key=\"X_feat\",\n",
    "    y_key=\"y_ret\",\n",
    "    hidden=(64, 32),\n",
    "    dropout=0.1,\n",
    "    lr=3e-4,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    seed=42\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    all_rows = []\n",
    "    week_metrics = []\n",
    "\n",
    "    for i in range(train_len, len(rolling)):\n",
    "        train_windows = rolling[i-train_len:i]\n",
    "        test_window   = rolling[i]\n",
    "\n",
    "        # --- build train panel ---\n",
    "        X_train, y_train, meta = panel_from_windows(train_windows, x_key=x_key, y_key=y_key)\n",
    "        if X_train.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # --- scaler on TRAIN only ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "        # --- build test cross-section ---\n",
    "        X_test_df = test_window[x_key]            # assets x features\n",
    "        y_test = test_window[y_key]\n",
    "        if not isinstance(y_test, pd.Series):\n",
    "            y_test = pd.Series(y_test, index=X_test_df.index)\n",
    "\n",
    "        assets = X_test_df.index.intersection(y_test.index).sort_values()\n",
    "        X_test = X_test_df.loc[assets].to_numpy(np.float32)\n",
    "        y_true = y_test.loc[assets].to_numpy(np.float32)\n",
    "\n",
    "        mask = np.isfinite(X_test).all(axis=1) & np.isfinite(y_true)\n",
    "        assets = assets[mask]\n",
    "        X_test = X_test[mask]\n",
    "        y_true = y_true[mask]\n",
    "\n",
    "        X_test_sc = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "        # --- train model (fresh each step) ---\n",
    "        model = build_mlp(in_dim=X_train_sc.shape[1], hidden=hidden, dropout=dropout, lr=lr)\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_sc, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "\n",
    "        # --- predict ---\n",
    "        y_pred = model.predict(X_test_sc, batch_size=1024, verbose=0).squeeze()\n",
    "\n",
    "        # --- baseline: sample mean of past period returns ---\n",
    "        y_pred_sm_ser = baseline_sample_mean(test_window).loc[assets]\n",
    "        y_pred_sm = y_pred_sm_ser.to_numpy(np.float32)\n",
    "\n",
    "        # store per-asset predictions\n",
    "        for a, yt, yp, ypsm in zip(assets, y_true, y_pred, y_pred_sm):\n",
    "            all_rows.append(\n",
    "                {\"window_idx\": i, \"asset\": a, \"y\": float(yt), \"pred_mlp\": float(yp), \"pred_sm\": float(ypsm)}\n",
    "            )\n",
    "\n",
    "        # per-window metrics (cross-section)\n",
    "        m_mlp = prediction_metrics(y_true, y_pred)\n",
    "        m_sm  = prediction_metrics(y_true, y_pred_sm)\n",
    "        week_metrics.append({\"window_idx\": i, **{f\"mlp_{k}\": v for k,v in m_mlp.items()},\n",
    "                                **{f\"sm_{k}\": v for k,v in m_sm.items()}})\n",
    "\n",
    "    preds_df = pd.DataFrame(all_rows)\n",
    "    week_df  = pd.DataFrame(week_metrics)\n",
    "\n",
    "    # pooled metrics over all (window, asset) test points\n",
    "    pooled_mlp = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_mlp\"])\n",
    "    pooled_sm  = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_sm\"])\n",
    "\n",
    "    return preds_df, week_df, pooled_mlp, pooled_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "394ebbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled MLP: {'MSE': 0.00097, 'MAE': 0.02018, 'R2': -2.47369, 'SignAcc': 0.4955, 'Corr': 0.03159}\n",
      "Pooled  SM: {'MSE': 0.0003, 'MAE': 0.01162, 'R2': -0.06393, 'SignAcc': 0.4982, 'Corr': -0.05135}\n",
      "Weekly win-rate (MAE): 0.13513513513513514\n"
     ]
    }
   ],
   "source": [
    "preds_df, week_df, pooled_mlp, pooled_sm = walk_forward_eval_mlp(\n",
    "    rolling=rolling,\n",
    "    train_len=100,   # e.g., last 52 periods (with days_per_week=2 that's ~104 trading days)\n",
    "    x_key=\"X_feat\",\n",
    "    y_key=\"y_ret\",\n",
    "    hidden=(16,8)\n",
    ")\n",
    "\n",
    "print(\"Pooled MLP:\", pooled_mlp)\n",
    "print(\"Pooled  SM:\", pooled_sm)\n",
    "print(\"Weekly win-rate (MAE):\", (week_df[\"mlp_MAE\"] < week_df[\"sm_MAE\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a76b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
