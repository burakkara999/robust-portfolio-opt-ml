{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "48d5911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "from asset_data_module import read_close_prices_all_merged\n",
    "from features import make_feature_windows\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "e1d2ad77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((981, 40), 430)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markets = ['dow30', 'commodities', 'bonds']\n",
    "# markets = ['dow30']\n",
    "# markets = ['commodities']\n",
    "# markets = ['bonds']\n",
    "start_date, end_date = \"2022-01-01\", \"2025-11-28\"\n",
    "# start_date, end_date = \"2024-06-01\", \"2025-11-28\"\n",
    "\n",
    "_, close_df = read_close_prices_all_merged(markets, after_date=start_date)\n",
    "close_df = close_df.loc[:end_date]\n",
    "\n",
    "rolling = make_feature_windows(\n",
    "    close_prices=close_df,\n",
    "    lookback=60,\n",
    "    horizon=1,\n",
    "    days_per_week=2\n",
    ")\n",
    "close_df.shape, len(rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "22a499ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow30:AAPL</th>\n",
       "      <th>dow30:AMGN</th>\n",
       "      <th>dow30:AXP</th>\n",
       "      <th>dow30:BA</th>\n",
       "      <th>dow30:CAT</th>\n",
       "      <th>dow30:CRM</th>\n",
       "      <th>dow30:CSCO</th>\n",
       "      <th>dow30:CVX</th>\n",
       "      <th>dow30:DIS</th>\n",
       "      <th>dow30:DOW</th>\n",
       "      <th>...</th>\n",
       "      <th>commodities:GC=F</th>\n",
       "      <th>commodities:HG=F</th>\n",
       "      <th>commodities:SI=F</th>\n",
       "      <th>bonds:EMB</th>\n",
       "      <th>bonds:HYG</th>\n",
       "      <th>bonds:IEF</th>\n",
       "      <th>bonds:LQD</th>\n",
       "      <th>bonds:SHY</th>\n",
       "      <th>bonds:TIP</th>\n",
       "      <th>bonds:TLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039733</td>\n",
       "      <td>-0.006861</td>\n",
       "      <td>0.020885</td>\n",
       "      <td>0.024756</td>\n",
       "      <td>0.059772</td>\n",
       "      <td>-0.115169</td>\n",
       "      <td>-0.040796</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>-0.010066</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>0.015673</td>\n",
       "      <td>-0.010146</td>\n",
       "      <td>-0.010286</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>-0.005046</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>-0.011982</td>\n",
       "      <td>-0.009609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015846</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.017646</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.014002</td>\n",
       "      <td>0.022731</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015242</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>-0.033202</td>\n",
       "      <td>-0.002904</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>-0.005612</td>\n",
       "      <td>-0.001055</td>\n",
       "      <td>-0.005530</td>\n",
       "      <td>-0.004629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016761</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>-0.019094</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011948</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>0.018231</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>-0.000235</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.009097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016645</td>\n",
       "      <td>-0.006606</td>\n",
       "      <td>-0.011988</td>\n",
       "      <td>0.035828</td>\n",
       "      <td>0.031550</td>\n",
       "      <td>-0.026799</td>\n",
       "      <td>-0.014047</td>\n",
       "      <td>-0.009185</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.025352</td>\n",
       "      <td>0.015187</td>\n",
       "      <td>-0.005464</td>\n",
       "      <td>-0.001971</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>0.005003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.013977</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>-0.041955</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>-0.011305</td>\n",
       "      <td>-0.029203</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>-0.020605</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004899</td>\n",
       "      <td>-0.035469</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>-0.018111</td>\n",
       "      <td>-0.005819</td>\n",
       "      <td>-0.014483</td>\n",
       "      <td>-0.018433</td>\n",
       "      <td>-0.002818</td>\n",
       "      <td>-0.009293</td>\n",
       "      <td>-0.029399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dow30:AAPL  dow30:AMGN  dow30:AXP  dow30:BA  dow30:CAT  dow30:CRM  \\\n",
       "0   -0.039733   -0.006861   0.020885  0.024756   0.059772  -0.115169   \n",
       "1   -0.015846    0.009504   0.017646  0.011340   0.020003   0.002807   \n",
       "2    0.016761    0.022147   0.005718  0.002410  -0.019094   0.028200   \n",
       "3   -0.016645   -0.006606  -0.011988  0.035828   0.031550  -0.026799   \n",
       "4   -0.013977    0.012826  -0.041955  0.004945   0.012564  -0.011305   \n",
       "\n",
       "   dow30:CSCO  dow30:CVX  dow30:DIS  dow30:DOW  ...  commodities:GC=F  \\\n",
       "0   -0.040796   0.024517  -0.010066   0.025691  ...          0.013907   \n",
       "1    0.014002   0.022731   0.016868   0.012772  ...         -0.015242   \n",
       "2    0.020082   0.023242   0.000380   0.001353  ...          0.011948   \n",
       "3   -0.014047  -0.009185  -0.015639   0.012760  ...          0.001429   \n",
       "4   -0.029203   0.020143  -0.020605   0.004328  ...         -0.004899   \n",
       "\n",
       "   commodities:HG=F  commodities:SI=F  bonds:EMB  bonds:HYG  bonds:IEF  \\\n",
       "0         -0.002268          0.015673  -0.010146  -0.010286  -0.004751   \n",
       "1         -0.000341         -0.033202  -0.002904  -0.002443  -0.005749   \n",
       "2          0.003967          0.018231  -0.001314   0.005459   0.002127   \n",
       "3          0.025352          0.015187  -0.005464  -0.001971   0.003358   \n",
       "4         -0.035469          0.014236  -0.018111  -0.005819  -0.014483   \n",
       "\n",
       "   bonds:LQD  bonds:SHY  bonds:TIP  bonds:TLT  \n",
       "0  -0.005046  -0.000702  -0.011982  -0.009609  \n",
       "1  -0.005612  -0.001055  -0.005530  -0.004629  \n",
       "2   0.002079  -0.000235   0.008205   0.009097  \n",
       "3   0.002459   0.000117  -0.006226   0.005003  \n",
       "4  -0.018433  -0.002818  -0.009293  -0.029399  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rolling[0]['past_weekly_returns'].shape)\n",
    "rolling[0]['past_weekly_returns'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "09518f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom_1w</th>\n",
       "      <th>mom_4w</th>\n",
       "      <th>mom_12w</th>\n",
       "      <th>vol_1w</th>\n",
       "      <th>vol_4w</th>\n",
       "      <th>sharpe_1w</th>\n",
       "      <th>sharpe_4w</th>\n",
       "      <th>vol_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bonds:EMB</th>\n",
       "      <td>-0.529241</td>\n",
       "      <td>-0.037816</td>\n",
       "      <td>-0.266307</td>\n",
       "      <td>-0.990466</td>\n",
       "      <td>-0.621157</td>\n",
       "      <td>-0.172261</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>-0.979524</td>\n",
       "      <td>0.254794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonds:HYG</th>\n",
       "      <td>-0.304409</td>\n",
       "      <td>0.087164</td>\n",
       "      <td>-0.134049</td>\n",
       "      <td>-1.223926</td>\n",
       "      <td>-1.124062</td>\n",
       "      <td>6.146437</td>\n",
       "      <td>0.527685</td>\n",
       "      <td>-1.426704</td>\n",
       "      <td>0.817271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonds:IEF</th>\n",
       "      <td>-0.735662</td>\n",
       "      <td>0.021070</td>\n",
       "      <td>-0.198222</td>\n",
       "      <td>-0.821394</td>\n",
       "      <td>-1.335768</td>\n",
       "      <td>-0.200501</td>\n",
       "      <td>0.557177</td>\n",
       "      <td>-0.217478</td>\n",
       "      <td>0.912569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonds:LQD</th>\n",
       "      <td>-0.495279</td>\n",
       "      <td>-0.021567</td>\n",
       "      <td>-0.108969</td>\n",
       "      <td>-0.940257</td>\n",
       "      <td>-1.363098</td>\n",
       "      <td>-0.174786</td>\n",
       "      <td>0.457339</td>\n",
       "      <td>-0.683379</td>\n",
       "      <td>0.521622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonds:SHY</th>\n",
       "      <td>-0.787964</td>\n",
       "      <td>-0.235119</td>\n",
       "      <td>-0.002339</td>\n",
       "      <td>-1.178144</td>\n",
       "      <td>-2.074935</td>\n",
       "      <td>-0.193058</td>\n",
       "      <td>0.770733</td>\n",
       "      <td>-0.877848</td>\n",
       "      <td>1.867246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mom_1w    mom_4w   mom_12w    vol_1w    vol_4w  sharpe_1w  \\\n",
       "bonds:EMB -0.529241 -0.037816 -0.266307 -0.990466 -0.621157  -0.172261   \n",
       "bonds:HYG -0.304409  0.087164 -0.134049 -1.223926 -1.124062   6.146437   \n",
       "bonds:IEF -0.735662  0.021070 -0.198222 -0.821394 -1.335768  -0.200501   \n",
       "bonds:LQD -0.495279 -0.021567 -0.108969 -0.940257 -1.363098  -0.174786   \n",
       "bonds:SHY -0.787964 -0.235119 -0.002339 -1.178144 -2.074935  -0.193058   \n",
       "\n",
       "           sharpe_4w  vol_ratio  max_drawdown  \n",
       "bonds:EMB   0.004759  -0.979524      0.254794  \n",
       "bonds:HYG   0.527685  -1.426704      0.817271  \n",
       "bonds:IEF   0.557177  -0.217478      0.912569  \n",
       "bonds:LQD   0.457339  -0.683379      0.521622  \n",
       "bonds:SHY   0.770733  -0.877848      1.867246  "
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['X_feat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "961e94f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonds:EMB   -0.007649\n",
       "bonds:HYG   -0.011056\n",
       "bonds:IEF    0.001292\n",
       "dtype: float64"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling[0]['y_ret'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "2d2947a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_from_windows(windows, x_key=\"past_weekly_returns\", y_key=\"y_ret\"):\n",
    "    X_list, y_list = [], []\n",
    "    meta_rows = []\n",
    "\n",
    "    for w_idx, w in enumerate(windows):\n",
    "        if x_key == 'past_weekly_returns':\n",
    "            X_df = w[x_key].T          # assets x n_lookback (weeks) (DataFrame)\n",
    "        elif x_key == 'X_feat':\n",
    "            X_df = w[x_key]          # assets x n_features (DataFrame)\n",
    "        y_ser = w[y_key]           # assets (Series or array)\n",
    "\n",
    "        if not isinstance(y_ser, pd.Series):\n",
    "            y_ser = pd.Series(y_ser, index=X_df.index)\n",
    "\n",
    "        assets = X_df.index.intersection(y_ser.index)\n",
    "        Xw = X_df.loc[assets].to_numpy(dtype=np.float32)\n",
    "        yw = y_ser.loc[assets].to_numpy(dtype=np.float32)\n",
    "\n",
    "        mask = np.isfinite(Xw).all(axis=1) & np.isfinite(yw)\n",
    "        Xw, yw = Xw[mask], yw[mask]\n",
    "        assets_kept = assets.to_numpy()[mask]\n",
    "\n",
    "        X_list.append(Xw)\n",
    "        y_list.append(yw)\n",
    "\n",
    "        t0, t1 = w.get(\"t0\", None), w.get(\"t1\", None)\n",
    "        for a in assets_kept:\n",
    "            meta_rows.append((w_idx, a, t0, t1))\n",
    "\n",
    "    X = np.vstack(X_list) ## weeks*assets x n_lookback/n_features\n",
    "    y = np.concatenate(y_list)\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"window_idx\", \"asset\", \"t0\", \"t1\"])\n",
    "    return X, y, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "00e8a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17200, 60),\n",
       " (17200,),\n",
       "    window_idx       asset         t0         t1\n",
       " 0           0  dow30:AAPL 2022-06-24 2022-06-28\n",
       " 1           0  dow30:AMGN 2022-06-24 2022-06-28\n",
       " 2           0   dow30:AXP 2022-06-24 2022-06-28\n",
       " 3           0    dow30:BA 2022-06-24 2022-06-28\n",
       " 4           0   dow30:CAT 2022-06-24 2022-06-28)"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_past_returns, y, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "X, y, meta = panel_from_windows(rolling, x_key='past_weekly_returns')\n",
    "# X, y, meta = panel_from_windows(rolling, x_key='X_feat')\n",
    "X.shape, y.shape, meta.head() ## len(rolling)*n_asset -- each row is a feature set -- to predict y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "4dcd561f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01450700405985117"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = rolling  # your rolling list\n",
    "\n",
    "W = meta[\"window_idx\"].nunique()\n",
    "split_w = int(0.8 * W)\n",
    "\n",
    "train_mask = (meta[\"window_idx\"] < split_w).values\n",
    "test_mask  = (meta[\"window_idx\"] >= split_w).values\n",
    "\n",
    "X_train_raw, y_train = X[train_mask], y[train_mask]\n",
    "X_test_raw,  y_test  = X[test_mask],  y[test_mask]\n",
    "X_past_returns_test_raw = X_past_returns[test_mask]\n",
    "\n",
    "# small validation from the tail of the training windows\n",
    "val_w = max(int(0.1 * split_w), 1)\n",
    "val_start = split_w - val_w\n",
    "val_mask = ((meta[\"window_idx\"] >= val_start) & (meta[\"window_idx\"] < split_w)).values\n",
    "tr2_mask = (meta[\"window_idx\"] < val_start).values\n",
    "\n",
    "X_tr_raw, y_tr = X[tr2_mask], y[tr2_mask]\n",
    "X_va_raw, y_va = X[val_mask], y[val_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr_raw).astype(np.float32)\n",
    "X_va = scaler.transform(X_va_raw).astype(np.float32)\n",
    "X_te = scaler.transform(X_test_raw).astype(np.float32)\n",
    "\n",
    "y_tr = y_tr.astype(np.float32)\n",
    "y_va = y_va.astype(np.float32)\n",
    "y_te = y_test.astype(np.float32)\n",
    "\n",
    "## SCALE Y\n",
    "y_mean = y_tr.mean()\n",
    "y_std  = y_tr.std() + 1e-8\n",
    "\n",
    "y_tr_s = ((y_tr - y_mean) / y_std).astype(np.float32)\n",
    "y_va_s = ((y_va - y_mean) / y_std).astype(np.float32)\n",
    "float(y_std)\n",
    "\n",
    "# X_tr = np.tanh(X_tr)\n",
    "# X_va = np.tanh(X_va)\n",
    "# X_te = np.tanh(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "c0b49e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12400, 60),\n",
       " (1360, 60),\n",
       " (3440, 60),\n",
       " (12400,),\n",
       " (1360,),\n",
       " (3440,),\n",
       " '86.0 test periods')"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_va.shape, X_te.shape, y_tr.shape, y_va.shape, y_te.shape, f\"{y_te.shape[0]/close_df.shape[1]} test periods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "0569f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3971 - mae: 0.8289 - val_loss: 1.3210 - val_mae: 0.7831\n",
      "Epoch 2/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - loss: 1.2883 - mae: 0.7908 - val_loss: 1.2687 - val_mae: 0.7626\n",
      "Epoch 3/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - loss: 1.2432 - mae: 0.7714 - val_loss: 1.2331 - val_mae: 0.7474\n",
      "Epoch 4/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - loss: 1.1735 - mae: 0.7476 - val_loss: 1.2095 - val_mae: 0.7371\n",
      "Epoch 5/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - loss: 1.1514 - mae: 0.7376 - val_loss: 1.1937 - val_mae: 0.7297\n",
      "Epoch 6/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - loss: 1.1364 - mae: 0.7331 - val_loss: 1.1805 - val_mae: 0.7234\n",
      "Epoch 7/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - loss: 1.1104 - mae: 0.7230 - val_loss: 1.1716 - val_mae: 0.7186\n",
      "Epoch 8/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step - loss: 1.0919 - mae: 0.7169 - val_loss: 1.1642 - val_mae: 0.7147\n",
      "Epoch 9/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - loss: 1.0783 - mae: 0.7095 - val_loss: 1.1579 - val_mae: 0.7116\n",
      "Epoch 10/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - loss: 1.0630 - mae: 0.7062 - val_loss: 1.1539 - val_mae: 0.7094\n",
      "Epoch 11/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 1.0617 - mae: 0.7024 - val_loss: 1.1502 - val_mae: 0.7075\n",
      "Epoch 12/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 1.0412 - mae: 0.6959 - val_loss: 1.1468 - val_mae: 0.7060\n",
      "Epoch 13/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 1.0457 - mae: 0.6959 - val_loss: 1.1440 - val_mae: 0.7045\n",
      "Epoch 14/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 1.0321 - mae: 0.6906 - val_loss: 1.1409 - val_mae: 0.7033\n",
      "Epoch 15/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 1.0304 - mae: 0.6909 - val_loss: 1.1388 - val_mae: 0.7023\n",
      "Epoch 16/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 1.0149 - mae: 0.6858 - val_loss: 1.1368 - val_mae: 0.7014\n",
      "Epoch 17/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 1.0085 - mae: 0.6842 - val_loss: 1.1351 - val_mae: 0.7007\n",
      "Epoch 18/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - loss: 1.0120 - mae: 0.6844 - val_loss: 1.1337 - val_mae: 0.6999\n",
      "Epoch 19/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 1.0119 - mae: 0.6831 - val_loss: 1.1323 - val_mae: 0.6992\n",
      "Epoch 20/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 1.0025 - mae: 0.6810 - val_loss: 1.1316 - val_mae: 0.6989\n",
      "Epoch 21/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - loss: 1.0013 - mae: 0.6794 - val_loss: 1.1295 - val_mae: 0.6983\n",
      "Epoch 22/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9989 - mae: 0.6792 - val_loss: 1.1293 - val_mae: 0.6980\n",
      "Epoch 23/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 0.9976 - mae: 0.6774 - val_loss: 1.1285 - val_mae: 0.6977\n",
      "Epoch 24/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.9903 - mae: 0.6755 - val_loss: 1.1277 - val_mae: 0.6975\n",
      "Epoch 25/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.9916 - mae: 0.6768 - val_loss: 1.1262 - val_mae: 0.6971\n",
      "Epoch 26/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9878 - mae: 0.6735 - val_loss: 1.1251 - val_mae: 0.6968\n",
      "Epoch 27/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 0.9849 - mae: 0.6730 - val_loss: 1.1241 - val_mae: 0.6965\n",
      "Epoch 28/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.9750 - mae: 0.6700 - val_loss: 1.1239 - val_mae: 0.6963\n",
      "Epoch 29/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - loss: 0.9789 - mae: 0.6711 - val_loss: 1.1235 - val_mae: 0.6963\n",
      "Epoch 30/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 0.9756 - mae: 0.6693 - val_loss: 1.1231 - val_mae: 0.6963\n",
      "Epoch 31/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 0.9698 - mae: 0.6680 - val_loss: 1.1228 - val_mae: 0.6964\n",
      "Epoch 32/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.9723 - mae: 0.6677 - val_loss: 1.1222 - val_mae: 0.6965\n",
      "Epoch 33/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9647 - mae: 0.6661 - val_loss: 1.1213 - val_mae: 0.6964\n",
      "Epoch 34/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.9642 - mae: 0.6657 - val_loss: 1.1214 - val_mae: 0.6965\n",
      "Epoch 35/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.9612 - mae: 0.6648 - val_loss: 1.1204 - val_mae: 0.6963\n",
      "Epoch 36/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - loss: 0.9570 - mae: 0.6634 - val_loss: 1.1203 - val_mae: 0.6963\n",
      "Epoch 37/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.9593 - mae: 0.6649 - val_loss: 1.1196 - val_mae: 0.6962\n",
      "Epoch 38/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 0.9643 - mae: 0.6659 - val_loss: 1.1188 - val_mae: 0.6960\n",
      "Epoch 39/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - loss: 0.9563 - mae: 0.6640 - val_loss: 1.1188 - val_mae: 0.6962\n",
      "Epoch 40/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.9494 - mae: 0.6625 - val_loss: 1.1186 - val_mae: 0.6963\n",
      "Epoch 41/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.9500 - mae: 0.6621 - val_loss: 1.1177 - val_mae: 0.6960\n",
      "Epoch 42/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9511 - mae: 0.6612 - val_loss: 1.1173 - val_mae: 0.6961\n",
      "Epoch 43/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.9427 - mae: 0.6601 - val_loss: 1.1170 - val_mae: 0.6961\n",
      "Epoch 44/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9467 - mae: 0.6605 - val_loss: 1.1164 - val_mae: 0.6960\n",
      "Epoch 45/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9425 - mae: 0.6594 - val_loss: 1.1166 - val_mae: 0.6960\n",
      "Epoch 46/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.9420 - mae: 0.6585 - val_loss: 1.1167 - val_mae: 0.6963\n",
      "Epoch 47/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - loss: 0.9439 - mae: 0.6592 - val_loss: 1.1168 - val_mae: 0.6964\n",
      "Epoch 48/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.9323 - mae: 0.6552 - val_loss: 1.1160 - val_mae: 0.6965\n",
      "Epoch 49/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9324 - mae: 0.6546 - val_loss: 1.1158 - val_mae: 0.6966\n",
      "Epoch 50/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 0.9286 - mae: 0.6542 - val_loss: 1.1148 - val_mae: 0.6966\n",
      "Epoch 51/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 0.9370 - mae: 0.6547 - val_loss: 1.1139 - val_mae: 0.6963\n",
      "Epoch 52/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.9297 - mae: 0.6545 - val_loss: 1.1140 - val_mae: 0.6966\n",
      "Epoch 53/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 0.9328 - mae: 0.6563 - val_loss: 1.1142 - val_mae: 0.6969\n",
      "Epoch 54/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.9214 - mae: 0.6525 - val_loss: 1.1144 - val_mae: 0.6970\n",
      "Epoch 55/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.9258 - mae: 0.6537 - val_loss: 1.1132 - val_mae: 0.6968\n",
      "Epoch 56/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.9195 - mae: 0.6524 - val_loss: 1.1130 - val_mae: 0.6969\n",
      "Epoch 57/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - loss: 0.9272 - mae: 0.6534 - val_loss: 1.1129 - val_mae: 0.6970\n",
      "Epoch 58/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 0.9197 - mae: 0.6509 - val_loss: 1.1137 - val_mae: 0.6976\n",
      "Epoch 59/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 0.9174 - mae: 0.6513 - val_loss: 1.1134 - val_mae: 0.6979\n",
      "Epoch 60/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.9207 - mae: 0.6522 - val_loss: 1.1134 - val_mae: 0.6981\n",
      "Epoch 61/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 0.9167 - mae: 0.6503 - val_loss: 1.1135 - val_mae: 0.6981\n",
      "Epoch 62/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.9092 - mae: 0.6476 - val_loss: 1.1136 - val_mae: 0.6985\n",
      "Epoch 63/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - loss: 0.9152 - mae: 0.6482 - val_loss: 1.1135 - val_mae: 0.6985\n",
      "Epoch 64/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 0.9159 - mae: 0.6500 - val_loss: 1.1130 - val_mae: 0.6982\n",
      "Epoch 65/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - loss: 0.9104 - mae: 0.6492 - val_loss: 1.1125 - val_mae: 0.6983\n",
      "Epoch 66/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - loss: 0.9133 - mae: 0.6492 - val_loss: 1.1122 - val_mae: 0.6983\n",
      "Epoch 67/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.9019 - mae: 0.6456 - val_loss: 1.1124 - val_mae: 0.6984\n",
      "Epoch 68/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - loss: 0.9088 - mae: 0.6478 - val_loss: 1.1118 - val_mae: 0.6984\n",
      "Epoch 69/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.9113 - mae: 0.6476 - val_loss: 1.1119 - val_mae: 0.6985\n",
      "Epoch 70/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.9101 - mae: 0.6483 - val_loss: 1.1115 - val_mae: 0.6986\n",
      "Epoch 71/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.9023 - mae: 0.6455 - val_loss: 1.1120 - val_mae: 0.6990\n",
      "Epoch 72/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - loss: 0.8992 - mae: 0.6454 - val_loss: 1.1122 - val_mae: 0.6989\n",
      "Epoch 73/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 0.9035 - mae: 0.6461 - val_loss: 1.1127 - val_mae: 0.6994\n",
      "Epoch 74/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 0.8946 - mae: 0.6430 - val_loss: 1.1132 - val_mae: 0.6997\n",
      "Epoch 75/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.8947 - mae: 0.6438 - val_loss: 1.1131 - val_mae: 0.6998\n",
      "Epoch 76/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.8960 - mae: 0.6442 - val_loss: 1.1126 - val_mae: 0.6996\n",
      "Epoch 77/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.8943 - mae: 0.6428 - val_loss: 1.1125 - val_mae: 0.6998\n",
      "Epoch 78/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.8954 - mae: 0.6438 - val_loss: 1.1123 - val_mae: 0.6996\n",
      "Epoch 79/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.8950 - mae: 0.6441 - val_loss: 1.1130 - val_mae: 0.6997\n",
      "Epoch 80/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.8936 - mae: 0.6448 - val_loss: 1.1141 - val_mae: 0.7002\n",
      "Epoch 81/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.8894 - mae: 0.6422 - val_loss: 1.1141 - val_mae: 0.7000\n",
      "Epoch 82/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 0.8854 - mae: 0.6413 - val_loss: 1.1139 - val_mae: 0.7001\n",
      "Epoch 83/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 0.8897 - mae: 0.6431 - val_loss: 1.1138 - val_mae: 0.6999\n",
      "Epoch 84/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.8894 - mae: 0.6416 - val_loss: 1.1136 - val_mae: 0.7000\n",
      "Epoch 85/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.8912 - mae: 0.6428 - val_loss: 1.1133 - val_mae: 0.7001\n",
      "Epoch 86/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 0.8866 - mae: 0.6409 - val_loss: 1.1131 - val_mae: 0.7000\n",
      "Epoch 87/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step - loss: 0.8839 - mae: 0.6400 - val_loss: 1.1134 - val_mae: 0.7004\n",
      "Epoch 88/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 0.8775 - mae: 0.6381 - val_loss: 1.1136 - val_mae: 0.7006\n",
      "Epoch 89/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - loss: 0.8836 - mae: 0.6410 - val_loss: 1.1130 - val_mae: 0.7005\n",
      "Epoch 90/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.8790 - mae: 0.6392 - val_loss: 1.1127 - val_mae: 0.7004\n",
      "Epoch 91/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.8787 - mae: 0.6383 - val_loss: 1.1136 - val_mae: 0.7007\n",
      "Epoch 92/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 0.8845 - mae: 0.6416 - val_loss: 1.1136 - val_mae: 0.7007\n",
      "Epoch 93/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.8746 - mae: 0.6394 - val_loss: 1.1142 - val_mae: 0.7011\n",
      "Epoch 94/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - loss: 0.8772 - mae: 0.6405 - val_loss: 1.1141 - val_mae: 0.7011\n",
      "Epoch 95/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.8750 - mae: 0.6372 - val_loss: 1.1137 - val_mae: 0.7010\n",
      "Epoch 96/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.8788 - mae: 0.6387 - val_loss: 1.1131 - val_mae: 0.7008\n",
      "Epoch 97/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 0.8720 - mae: 0.6354 - val_loss: 1.1140 - val_mae: 0.7010\n",
      "Epoch 98/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 0.8753 - mae: 0.6383 - val_loss: 1.1137 - val_mae: 0.7011\n",
      "Epoch 99/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.8681 - mae: 0.6362 - val_loss: 1.1144 - val_mae: 0.7013\n",
      "Epoch 100/100\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 0.8780 - mae: 0.6395 - val_loss: 1.1151 - val_mae: 0.7014\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(in_dim, hidden=(64, 32), dropout=0.1, lr=1e-3):\n",
    "    inputs = keras.Input(shape=(in_dim,))\n",
    "    x = inputs\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation=\"relu\",)(x) #kernel_regularizer=regularizers.l2(1e-4)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_mlp(in_dim=X_tr.shape[1], hidden=(32, 16), dropout=0.1, lr=1e-4)\n",
    "# model = build_mlp(in_dim=X_tr.shape[1], hidden=(15, 15, 15, 15, 15, 15), dropout=0.0, lr=1e-4)\n",
    "# model = build_mlp(in_dim=X_tr.shape[1], hidden=(15, 15, 15, 15, 15, 15), dropout=0.0, lr=0.01)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=100, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    # X_tr, y_tr,\n",
    "    X_tr, y_tr_s,   ## Scaled y\n",
    "    # validation_data=(X_va, y_va),\n",
    "    validation_data=(X_va, y_va_s),   ## Scaled y\n",
    "    # epochs=150,\n",
    "    epochs=100,\n",
    "    # batch_size=256,\n",
    "    batch_size=100,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "55b4068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def prediction_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    sign_acc = (np.sign(y_true) == np.sign(y_pred)).mean()\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan\n",
    "    \n",
    "    return {\"MSE\": round(mse, 5), \"MAE\": round(mae, 5), \"R2\": round(r2, 5), \"SignAcc\": round(float(sign_acc), 5), \"Corr\": round(float(corr), 5)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "841800ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean\n",
      "{'MSE': 0.0004, 'MAE': 0.01182, 'R2': -0.04956, 'SignAcc': 0.4939, 'Corr': -0.08761}\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "MLP\n",
      "{'MSE': 0.0004, 'MAE': 0.01197, 'R2': -0.04626, 'SignAcc': 0.49913, 'Corr': -0.00296}\n",
      "0.001889708\n"
     ]
    }
   ],
   "source": [
    "y_pred_sm = X_past_returns_test_raw.astype(np.float32).mean(axis=1)\n",
    "# X_te_raw = scaler.inverse_transform(X_te)\n",
    "# y_pred_sm = X_te_raw.mean(axis=1)\n",
    "print(\"Sample Mean\")\n",
    "print(prediction_metrics(y_te, y_pred_sm))\n",
    "\n",
    "# y_pred = model.predict(X_te, batch_size=1024).squeeze()\n",
    "## Scaled y:\n",
    "pred_s = model.predict(X_te, batch_size=1024).squeeze()\n",
    "y_pred = y_mean + y_std * pred_s\n",
    "\n",
    "print(\"MLP\")\n",
    "print(prediction_metrics(y_te, y_pred))\n",
    "\n",
    "print(y_te.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "75af9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_sample_mean(test_window):\n",
    "    # past_weekly_returns: (lookback periods) x (assets)\n",
    "    return test_window[\"past_weekly_returns\"].mean(axis=0)  # pd.Series indexed by asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35858728",
   "metadata": {},
   "source": [
    "Walk-Forward Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "f2307a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_eval_mlp(\n",
    "    rolling,\n",
    "    train_len=150,          # number of windows to train on each step\n",
    "    x_key=\"X_feat\",\n",
    "    y_key=\"y_ret\",\n",
    "    hidden=(64, 32),\n",
    "    dropout=0.1,\n",
    "    lr=3e-4,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    seed=42\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    all_rows = []\n",
    "    week_metrics = []\n",
    "\n",
    "    for i in range(train_len, len(rolling)):\n",
    "        train_windows = rolling[i-train_len:i]\n",
    "        test_window   = rolling[i]\n",
    "\n",
    "        # --- build train panel ---\n",
    "        X_train, y_train, meta = panel_from_windows(train_windows, x_key=x_key, y_key=y_key)\n",
    "        if X_train.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # --- scaler on TRAIN only ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "        # --- build test cross-section ---\n",
    "        X_test_df = test_window[x_key]            # assets x features\n",
    "        y_test = test_window[y_key]\n",
    "        if not isinstance(y_test, pd.Series):\n",
    "            y_test = pd.Series(y_test, index=X_test_df.index)\n",
    "\n",
    "        assets = X_test_df.index.intersection(y_test.index).sort_values()\n",
    "        X_test = X_test_df.loc[assets].to_numpy(np.float32)\n",
    "        y_true = y_test.loc[assets].to_numpy(np.float32)\n",
    "\n",
    "        mask = np.isfinite(X_test).all(axis=1) & np.isfinite(y_true)\n",
    "        assets = assets[mask]\n",
    "        X_test = X_test[mask]\n",
    "        y_true = y_true[mask]\n",
    "\n",
    "        X_test_sc = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "        # --- train model (fresh each step) ---\n",
    "        model = build_mlp(in_dim=X_train_sc.shape[1], hidden=hidden, dropout=dropout, lr=lr)\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_sc, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "\n",
    "        # --- predict ---\n",
    "        y_pred = model.predict(X_test_sc, batch_size=1024, verbose=0).squeeze()\n",
    "\n",
    "        # --- baseline: sample mean of past period returns ---\n",
    "        y_pred_sm_ser = baseline_sample_mean(test_window).loc[assets]\n",
    "        y_pred_sm = y_pred_sm_ser.to_numpy(np.float32)\n",
    "\n",
    "        # store per-asset predictions\n",
    "        for a, yt, yp, ypsm in zip(assets, y_true, y_pred, y_pred_sm):\n",
    "            all_rows.append(\n",
    "                {\"window_idx\": i, \"asset\": a, \"y\": float(yt), \"pred_mlp\": float(yp), \"pred_sm\": float(ypsm)}\n",
    "            )\n",
    "\n",
    "        # per-window metrics (cross-section)\n",
    "        m_mlp = prediction_metrics(y_true, y_pred)\n",
    "        m_sm  = prediction_metrics(y_true, y_pred_sm)\n",
    "        week_metrics.append({\"window_idx\": i, **{f\"mlp_{k}\": v for k,v in m_mlp.items()},\n",
    "                                **{f\"sm_{k}\": v for k,v in m_sm.items()}})\n",
    "\n",
    "    preds_df = pd.DataFrame(all_rows)\n",
    "    week_df  = pd.DataFrame(week_metrics)\n",
    "\n",
    "    # pooled metrics over all (window, asset) test points\n",
    "    pooled_mlp = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_mlp\"])\n",
    "    pooled_sm  = prediction_metrics(preds_df[\"y\"], preds_df[\"pred_sm\"])\n",
    "\n",
    "    return preds_df, week_df, pooled_mlp, pooled_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "394ebbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_df, week_df, pooled_mlp, pooled_sm = walk_forward_eval_mlp(\n",
    "#     rolling=rolling,\n",
    "#     train_len=100,   # e.g., last 52 periods (with days_per_week=2 that's ~104 trading days)\n",
    "#     x_key=\"X_feat\",\n",
    "#     y_key=\"y_ret\",\n",
    "#     hidden=(16,8)\n",
    "# )\n",
    "\n",
    "# print(\"Pooled MLP:\", pooled_mlp)\n",
    "# print(\"Pooled  SM:\", pooled_sm)\n",
    "# print(\"Weekly win-rate (MAE):\", (week_df[\"mlp_MAE\"] < week_df[\"sm_MAE\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "3f9a76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_export_mlp(\n",
    "    rolling,\n",
    "    markets: list[str],\n",
    "    model_name: str = \"MLP\",\n",
    "    year: int = 2025,\n",
    "    train_len: int = 200,\n",
    "    x_key: str = \"past_weekly_returns\",\n",
    "    y_key: str = \"y_ret\",\n",
    "    hidden=(32, 16),\n",
    "    dropout: float = 0.1,\n",
    "    lr: float = 3e-4,\n",
    "    epochs: int = 150,\n",
    "    batch_size: int = 256,\n",
    "    seed: int = 42,\n",
    "    out_dir: str = \"data/prediction\",\n",
    "    date_key_candidates=(\"t1\", \"t0\", \"date\", \"Date\"),\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    markets_str = \"-\".join(markets)\n",
    "\n",
    "    rows_pred = []\n",
    "    rows_true = []\n",
    "\n",
    "    def get_window_date(w):\n",
    "        for k in date_key_candidates:\n",
    "            if k in w:\n",
    "                d = pd.to_datetime(w[k])\n",
    "                return d\n",
    "        return None  # if no date info\n",
    "\n",
    "    # find first index in 2025 (so we \"start walking forward on 2025\")\n",
    "    first_2025_idx = None\n",
    "    for i in range(len(rolling)):\n",
    "        d = get_window_date(rolling[i])\n",
    "        if isinstance(d, pd.Timestamp) and d.year == year:\n",
    "            first_2025_idx = i\n",
    "            break\n",
    "\n",
    "    if first_2025_idx is None:\n",
    "        raise ValueError(f\"No windows found with year={year}. Check your rolling windows date keys.\")\n",
    "\n",
    "    # walk-forward loop: start at first 2025 window, but still need train_len history\n",
    "    start_i = max(train_len, first_2025_idx)\n",
    "    print(\"START i \")\n",
    "    print(first_2025_idx)\n",
    "    print(start_i)\n",
    "    print(len(rolling))\n",
    "    for i in range(start_i, len(rolling)):\n",
    "        train_windows = rolling[i-train_len:i]\n",
    "        test_window   = rolling[i]\n",
    "\n",
    "        period = get_window_date(test_window)\n",
    "        if isinstance(period, pd.Timestamp):\n",
    "            if period.year != year:\n",
    "                continue\n",
    "        else:\n",
    "            # if no dates, we cannot filter by year; safest is to skip\n",
    "            continue\n",
    "\n",
    "        # --- build train panel ---\n",
    "        X_train, y_train, _ = panel_from_windows(train_windows, x_key=x_key, y_key=y_key)\n",
    "        if X_train.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # --- scaler on TRAIN only ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "        # --- build test cross-section ---\n",
    "        X_test_df = test_window[x_key].T  # assets x features\n",
    "        y_test = test_window[y_key]\n",
    "        if not isinstance(y_test, pd.Series):\n",
    "            y_test = pd.Series(y_test, index=X_test_df.index)\n",
    "\n",
    "        assets = X_test_df.index.intersection(y_test.index).sort_values()\n",
    "        if len(assets) == 0:\n",
    "            continue\n",
    "\n",
    "        X_test = X_test_df.loc[assets].to_numpy(np.float32)\n",
    "        y_true = y_test.loc[assets].to_numpy(np.float32)\n",
    "\n",
    "        mask = np.isfinite(X_test).all(axis=1) & np.isfinite(y_true)\n",
    "        assets = assets[mask]\n",
    "        X_test = X_test[mask]\n",
    "        y_true = y_true[mask]\n",
    "\n",
    "        if len(assets) == 0:\n",
    "            continue\n",
    "\n",
    "        X_test_sc = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "        # --- train model (fresh each step) ---\n",
    "        keras.backend.clear_session()\n",
    "        model = build_mlp(in_dim=X_train_sc.shape[1], hidden=hidden, dropout=dropout, lr=lr)\n",
    "\n",
    "        # (optional) early stopping; monitor val_loss only if you pass validation_data\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_sc, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "\n",
    "        # --- predict ---\n",
    "        y_pred = model.predict(X_test_sc, batch_size=1024, verbose=0).squeeze()\n",
    "\n",
    "        # store long rows (period, asset)\n",
    "        for a, yt, yp in zip(assets, y_true, y_pred):\n",
    "            rows_true.append({\"period\": period, \"asset\": a, \"true\": float(yt)})\n",
    "            rows_pred.append({\"period\": period, \"asset\": a, \"pred\": float(yp)})\n",
    "\n",
    "    # --- long -> wide ---\n",
    "    pred_long = pd.DataFrame(rows_pred)\n",
    "    true_long = pd.DataFrame(rows_true)\n",
    "\n",
    "    if pred_long.empty:\n",
    "        raise ValueError(\"No predictions were produced. Check train_len, date keys, or rolling coverage.\")\n",
    "\n",
    "    expected_df = pred_long.pivot(index=\"period\", columns=\"asset\", values=\"pred\").sort_index()\n",
    "    true_df     = true_long.pivot(index=\"period\", columns=\"asset\", values=\"true\").sort_index()\n",
    "\n",
    "    # align columns (union) so shapes match\n",
    "    all_cols = expected_df.columns.union(true_df.columns)\n",
    "    expected_df = expected_df.reindex(columns=all_cols)\n",
    "    true_df     = true_df.reindex(columns=all_cols)\n",
    "\n",
    "    errors_df = true_df - expected_df  # error = true - pred\n",
    "\n",
    "    # --- save ---\n",
    "    p_exp = os.path.join(out_dir, f\"{model_name}_{markets_str}_expected_returns.csv\")\n",
    "    p_true = os.path.join(out_dir, f\"{model_name}_{markets_str}_true_returns.csv\")\n",
    "    p_err = os.path.join(out_dir, f\"{model_name}_{markets_str}_errors.csv\")\n",
    "\n",
    "    expected_df.to_csv(p_exp)\n",
    "    true_df.to_csv(p_true)\n",
    "    errors_df.to_csv(p_err)\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(\" \", p_exp, expected_df.shape)\n",
    "    print(\" \", p_true, true_df.shape)\n",
    "    print(\" \", p_err, errors_df.shape)\n",
    "\n",
    "    return expected_df, true_df, errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "b2048a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START i \n",
      "316\n",
      "316\n",
      "430\n",
      "Saved:\n",
      "  data/prediction/MLP_dow30-commodities-bonds_expected_returns.csv (114, 40)\n",
      "  data/prediction/MLP_dow30-commodities-bonds_true_returns.csv (114, 40)\n",
      "  data/prediction/MLP_dow30-commodities-bonds_errors.csv (114, 40)\n"
     ]
    }
   ],
   "source": [
    "expected_2025, true_2025, errors_2025 = walk_forward_export_mlp(\n",
    "    rolling=rolling,\n",
    "    markets=markets,     \n",
    "    model_name=\"MLP\",\n",
    "    year=2025,\n",
    "    train_len=200,\n",
    "    x_key=\"past_weekly_returns\",\n",
    "    y_key=\"y_ret\",\n",
    "    hidden=(32, 16),\n",
    "    dropout=0.1,\n",
    "    lr=1e-4,\n",
    "    epochs=150,\n",
    "    batch_size=256,\n",
    "    out_dir=\"data/prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea1254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
